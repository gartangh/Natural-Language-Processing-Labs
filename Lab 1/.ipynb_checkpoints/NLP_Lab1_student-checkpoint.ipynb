{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfIMdyAFLl3X"
   },
   "source": [
    "# Machine-Learning Based Natural Language Processing\n",
    "# Lab 1 - Text Processing and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2oVKUgNLl3Y"
   },
   "source": [
    "---\n",
    "\n",
    "## Outline\n",
    "\n",
    "In this session, we will teach you how to approach processing a text-based dataset, with the goal of training a classifier, based on classical machine learning techniques. We've chosen __sentiment analysis__ as an application.\n",
    "\n",
    "We will perform some preprocessing on data, see how to transform textual data into feature vectors, how to train several algorithms, as well as discuss evaluating the trained models.  We will make use of the sklearn machine learning library in python.  \n",
    "\n",
    "These are the topics we will cover:\n",
    "\n",
    "* ___Part 1 - Python for data (pre-)processing___\n",
    " * Basic data analytics\n",
    " * Text processing\n",
    "* ___Part 2 - Sentiment Analysis___\n",
    " * Data Exploration\n",
    " * Text processing\n",
    " * Feature Extraction - __Exercise 1__\n",
    " * Classification Models - __Exercise 2__\n",
    " * Evaluation\n",
    " * Regularization - __Exercise 3__\n",
    " * Kaggle competition (*)\n",
    "\n",
    "(\\*) As a **bonus**, we'll have a friendly kaggle competition to see who can get the highest performance for sentiment prediction on a held out test set. We want you to critically think about how to improve and generalize your model's performance!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3WZdFjXLl3a"
   },
   "source": [
    "# Part 1 - Python for data (pre)processing\n",
    "---\n",
    "Python is the most widely used programming language among data scientists and machine learning engineers, next to R, Matlab and SQL (although the latter is not a programming language in the strict sense). It is very readable and development times are usually quite fast, which makes it the perfect language to do rapid prototyping and to toy around with your ideas. It has also become an entire ecosystem on its own among data scientist: a plethora of GitHub data science repositories exist, which you can use in your own projects; if you have some data science problem, chances are people have already worked on it in the past and have open-sourced their code. And finally, Python is the language of choice if you want to experiment with deep learning -- the latest 'big leap' in machine learning -- since the most mature and most widely used frameworks PyTorch, Tensorflow and Keras provide very easy-to-use Python interfaces.\n",
    "\n",
    "In this lab session, we will explore Python for (advanced) data processing and data analytics. We will focus on core Python modules and programming paradigms. The motivation is that, once you have good knowledge of the core Python modules, external modules such as NumPy, SciPy, Pandas, Seaborn... become much easier to learn and work with.\n",
    "\n",
    "Today, we will use the _Hillary Clinton and Donald Trump 2016 US Presidential Election Twitter dataset_, an open dataset provided by Kaggle, containing ca. 6500 tweets. We will cover the following topics:\n",
    "* Read the dataset in memory\n",
    "* Calculate basic analytics on the dataset\n",
    "* Make visual plots of dataset metrics\n",
    "* Perform text cleaning on the tweets\n",
    "* Identify distinct words in the tweets\n",
    "* Create a basic AI that can auto-generate a tweet\n",
    "\n",
    "**Remark**: this lab session is self-paced, and we hugely encourage that you go out on the internet and become big friends with _Google_ and _StackOverflow_. No one has complete knowledge of the entire Python core, let alone knowing the most efficient methods to achieve a certain result. If you have a basic Python problem (e.g., \"how would I initialize a list of all zeros with a given length?\") you will definitely and easily find multiple answers on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUFy-l8CLl3b"
   },
   "source": [
    "## 1. Basic data analytics\n",
    "We have provided the dataset in csv format. Pandas will be used to read the dataset into memory. Pandas is a convenient module to visualize tabular datasets, and we will use it now in the beginning of this lab session to get a quick look at the data. Read the csv file using Pandas and visualize it in this notebook. You can also take a look at the official dataset website for more details on the dataset: https://www.kaggle.com/benhamner/clinton-trump-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "colab_type": "code",
    "id": "qRUXyT2NcxmE",
    "outputId": "ce81244f-7fe4-4aa4-ba46-1642e92772f1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.dropbox.com/s/octyjwtn4gwv7gm/data.zip?dl=1\" -O data.zip\n",
    "!unzip -o data.zip\n",
    "!rm data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "colab_type": "code",
    "id": "3e2msn1uLl3c",
    "outputId": "752bdd3e-e03f-43ac-9dcc-2cc6be96a2b3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4b9396a1254e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# read the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweets_stripped.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# read the data\n",
    "dataset = pandas.read_csv('tweets_stripped.csv', sep=',', header=0)\n",
    "\n",
    "# show the 100th datapoint\n",
    "dataset.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S9_9x8ZULl3h"
   },
   "source": [
    "We will now use Python's own built-in `csv` module to read and process the csv file. To store the tweets in memory, we will write our own Tweet class in which we can store the data in a structured fashion. Study the code of the Tweet class below; we have already written the constructor `__init__` method that stores all necessary data as atrributes in the object.\n",
    "\n",
    "Complete the `__str__()` function that can be used to \"pretty print\" a Tweet object (e.g., print its ID, handle and text in a nice format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B_zGYp1gLl3i",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, tweet_id, handle, text, is_retweet, original_author, time, lang, retweet_count, favorite_count):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.handle = handle\n",
    "        self.text = text\n",
    "        self.is_retweet = is_retweet == 'True'\n",
    "        self.original_author = original_author\n",
    "        self.time = time\n",
    "        self.lang = lang\n",
    "        self.retweet_count = int(retweet_count)\n",
    "        self.favorite_count = int(favorite_count)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.tweet_id + ' / ' + self.handle + \": \" + self.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8drRPsPBLl3m"
   },
   "source": [
    "Now use the `csv` module to read the csv dataset row by row. For each row, create a Tweet object and store it in a list. This list will be our dataset. Use the official Python docs as inspiration: https://docs.python.org/3.6/library/csv.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0TQDxF52Ll3o",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "dataset = []\n",
    "\n",
    "with open('tweets_stripped.csv', 'r') as data_file:\n",
    "    datareader = csv.reader(data_file, delimiter=',', quotechar='\"')\n",
    "    for i, row in enumerate(datareader):\n",
    "        if i > 0:\n",
    "            t = Tweet(row[0], row[1], row[2], row[3], row[4], row[5], row[10], row[11], row[12])\n",
    "            dataset.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b3yJQijiLl3s"
   },
   "source": [
    "### 1.1 Warm-up questions\n",
    "Again, as a reminder, Google and StackOverflow are your friends!\n",
    "\n",
    "**Question:** How many tweets are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7mPUqyMGLl3t",
    "outputId": "f12dfadc-6335-4d99-ec39-4743370a8a9a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOgdYahvLl3y"
   },
   "source": [
    "**Question:** How many of these tweets are retweets? Give an absolute number and percentage. Also: what percentage of tweets are from Hillary and from Trump?\n",
    "\n",
    "For this question you can use for-loops, but Python has a very compact and powerful syntax called a _list comprehension_. Toy around with the examples below, and then tackle the question.\n",
    "\n",
    "Alternatively, use a pandas dataframe (see, e.g., https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "1z2OrlOPLl30",
    "outputId": "97612dce-bfc2-45b7-c3a7-99c7ddc60797",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "print(a)\n",
    "b = [(x + 1) for x in a]\n",
    "print(b)\n",
    "c = [x**2 for x in range(10)]\n",
    "print(c)\n",
    "print(sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "UwUc0OQyLl33",
    "outputId": "f06971f7-6812-465e-d103-1baa8293c127",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaP_pee7Ll37"
   },
   "source": [
    "**Question 3.** Which tweet was the most popular in terms of retweets? And what is the top 3 most popular tweets? Once found, you can look up the original tweet on Twitter by filling in the correct handle and id in the following URL: https://twitter.com/{handle}/status/{id}\n",
    "\n",
    "For this question, you can also use for loops to find the most popular tweet. Python, however, has good support for sorting datastructures. Take a look below how the built-in `sorted()` function works. (Alternatively, use pandas `sort_values()` on the pandas dataframe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "20mU8BZDLl38",
    "outputId": "192c1e94-7aa5-4b48-e299-0cbb5b9cce48",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a = ['bbbbb', 'ee', 'aaa', 'cccc', 'd']\n",
    "print(sorted(a))\n",
    "print(sorted(a, reverse=True))\n",
    "print(sorted(a, key=lambda x:len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "GCyBePsxLl4A",
    "outputId": "0555804b-b522-481e-e0dc-988e9331ba7d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWsk9IFDLl4E"
   },
   "source": [
    "**Question:** Who (Trump or Hillary) gets the most retweets on average? And based on the median value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "P8YbOsjULl4F",
    "outputId": "cf62ed9b-f3c7-4f99-cd41-33890c936b31",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ0tJeBcLl4I"
   },
   "source": [
    "### 1.2 Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Z_YCm6NLl4K"
   },
   "source": [
    "We'll now illustrate some visualizations of the data with Matplotlib and Pyplot. Matplotlib is the fundamental visualisation tool in Python, and Pyplot is an abstraction layer on top of Matplotlib in such a way that Matlab users feel right at home. These modules are, however, not a part of the core Python framework and should be installed separately.\n",
    "\n",
    "Below we show the needed imports. In order to visualise the plots in the notebook, we use a jupyter-specific syntax `%matplotlib inline` to tell the engine to render the images right within the notebook.\n",
    "\n",
    "(Note: As an alternative to matplotlib, especially for people familiar with [ggplot](https://ggplot2.tidyverse.org/) in R, you may want to explore, e.g., the [plotnine library](https://plotnine.readthedocs.io/en/stable/#) in python.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bxP4fcqELl4L",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kk4pS7AjLl4O"
   },
   "source": [
    "Below we show some basic plotting examples that you can play with, but you must realise that plotting is always a matter of trial and error, and requires frequent searches on Google and StackOverflow for existing recipes that you can use.\n",
    "\n",
    "##### Line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "t5TYW2KtLl4R",
    "outputId": "dbd089a0-0593-4ed4-f84e-44df85b55316",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "x = range(10)\n",
    "y = [i**2 for i in x]\n",
    "yy = [i**2.2 for i in x]\n",
    "\n",
    "plt.clf()         # clear figure\n",
    "plt.plot(x, y)    # draw a plot\n",
    "plt.plot(x, yy)   # draw another plot\n",
    "plt.show()        # show the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y3s509VlLl4V"
   },
   "source": [
    "##### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "colab_type": "code",
    "id": "Z393nvfELl4X",
    "outputId": "852717bc-ab4a-41b8-814c-24ce5770c211",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "d = [random.gauss(0, 1) for i in range(5000)]\n",
    "e = [random.gauss(2, 1) for i in range(5000)]\n",
    "\n",
    "plt.clf()\n",
    "# draw histogram, give them labels, set opacity to 0.5\n",
    "plt.hist(d, bins=30, label='mu 0', alpha=0.5)\n",
    "plt.hist(e, bins=30, label='mu 2', alpha=0.5)\n",
    "# draw legend\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ql-NM8eYLl4a"
   },
   "source": [
    "##### Bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "oQE5vbJ-Ll4b",
    "outputId": "cb5175e7-1f95-4817-f0ae-48d43fccd457",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_values = ['N-VA', 'CD&V', 'Groen', 'Open VLD', 'SP.A', 'Vl Belang', 'PVDA']\n",
    "y_values = [27.9, 14.7, 14.6, 14.2, 12.7, 9.3, 5.9]\n",
    "colors = ['yellow', 'orange', 'green', 'blue', 'red', 'brown', 'purple']\n",
    "\n",
    "plt.clf()\n",
    "# set the plot title\n",
    "plt.title(\"Peiling April 2019\")\n",
    "# create a bar chart\n",
    "barlist = plt.bar(x=x_values, width=0.5, height=y_values)\n",
    "# set the colors of each bar\n",
    "for i in range(len(x_values)):\n",
    "    barlist[i].set_color(colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9_hXFBPLl4e"
   },
   "source": [
    "### Visualisation exercises\n",
    "Again, Google and StackOverflow are your friends!\n",
    "\n",
    "Example 1. Generate a line plot of the tweet volume per each month for each candidate.\n",
    "\n",
    "To get the month in which a tweet was sent, take a look at Python's powerful `datetime` module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "tDJ_twr_Ll4f",
    "outputId": "5b476436-c7ee-4eba-fcf2-1ca67c52b789",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "# Code for Example 1\n",
    "\n",
    "tweet_volume = {\n",
    "    'HillaryClinton': [0]*12,\n",
    "    'realDonaldTrump': [0]*12\n",
    "}\n",
    "\n",
    "for t in dataset:\n",
    "    tweet_month = dt.datetime.strptime(t.time, '%Y-%m-%dT%H:%M:%S').month - 1\n",
    "    tweet_volume[t.handle][tweet_month] += 1\n",
    "    \n",
    "plt.clf()\n",
    "plt.plot(tweet_volume['HillaryClinton'], label='Clinton')\n",
    "plt.plot(tweet_volume['realDonaldTrump'], label='Trump')\n",
    "plt.xticks(range(0,12), ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyWfXIlFLl4k"
   },
   "source": [
    "Example 2. Do the same, but now use a bar chart. Example:\n",
    "<img src=\"data:image/png;base64,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\" style=\"width: 300px;\"/>\n",
    "\n",
    "You will notice that preparing the x-values is quite cumbersome and not flexible enough. That's why we will use histograms in the next assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "fr161nieLl4l",
    "outputId": "97934688-1132-4fe9-d24b-cc583e56c2c7",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Example 2:\n",
    "\n",
    "x_values = range(0, 12)\n",
    "\n",
    "plt.clf()\n",
    "plt.bar(x=x_values, width=0.3, height=tweet_volume['HillaryClinton'], label='Clinton')\n",
    "plt.bar(x=[k+0.3 for k in x_values], width=0.3, height=tweet_volume['realDonaldTrump'], label='Trump')\n",
    "\n",
    "plt.xticks([k+0.15 for k in x_values], ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC'])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0cdOoG0Ll4p"
   },
   "source": [
    "Example 3: Maybe dividing the time axis on a monthly level is too coarse. Histograms have the powerful capacity of easily tuning the number of bins, so that we can go as coarse or as fine-grained as we want. We now create a histogram of tweet volume as a function of time, per candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "RaZbYwgZLl4p",
    "outputId": "00fdf2ab-754f-44b4-8ac0-c65aec563d12",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Example 3:\n",
    "\n",
    "tweet_dates = {\n",
    "    'HillaryClinton': [],\n",
    "    'realDonaldTrump': []\n",
    "}\n",
    "\n",
    "for t in dataset:\n",
    "    tweet_date = dt.datetime.strptime(t.time, '%Y-%m-%dT%H:%M:%S')\n",
    "    tweet_dates[t.handle].append(tweet_date)\n",
    "    \n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist([tweet_dates['HillaryClinton'], tweet_dates['realDonaldTrump']], bins=30)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTy3Hk_SLl4t"
   },
   "source": [
    "**Question:** What are the Twitter accounts that Hillary Clinton retweets? Create a bar chart for the accounts that were retweeted at least 5 times. Create the same graph for Trump. Who is retweeting most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "j5DpV9yvLl4u",
    "outputId": "57e8039f-4210-4874-f80c-0e74fc3f5ee8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q4 (part 1: Hillary Clinton)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "2GiR6HUcLl4x",
    "outputId": "4b28fce4-11b4-49aa-9b15-0f719414b72f",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer Q4 (part 2: Donald Trump)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgvljryBLl40"
   },
   "source": [
    "## 2. Text Processing\n",
    "### 2.1 Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2AWYCPXwLl41"
   },
   "source": [
    "In the remainder of the lab session, we will focus on textual data. Performing analytics, business intelligence, machine learning, etc. on (written) text is called _Natural Language Processing (NLP)_ in academics. In the next few exercises we will learn the necessary skills and steps in preparing and transforming text into useful data that can be used in a wide variety of powerful machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HHJrNVLWLl42"
   },
   "source": [
    "### 2.1.1 Theory\n",
    "The very first step in NLP is _preprocessing_, that is, preparing the raw textual data such that we get rid of (most of the) noise and retain the most informative signal. Consider for example the following tweet, and how we can apply multiple possible preprocessing steps to arrive at a noise-free tweet. __All steps are optional__ and one should always consider the application at hand to determine which preprocessing steps are needed!\n",
    "\n",
    "> HEY @UGentstudent! This isn't a clean #NLP tweet :-D http://www.ugent.be\n",
    "\n",
    "__Step 1__ - Convert all characters to lowercase. By doing this, the words _'This'_ and _'this'_ become the same, which is what we want.\n",
    "\n",
    "> hey @ugentstudent! this isn't a clean #nlp tweet :-d http://www.ugent.be\n",
    "\n",
    "__Step 2__ - Apply normalization, for example:\n",
    " * Replacing all numbers by a single character, e.g. 0.\n",
    " * Replacing all characters with accents by their clean counterpart, e.g. √© becomes e.\n",
    " * Expanding popular contractions such as _\"isn't\"_ to _\"is not\"_.\n",
    " * ...\n",
    "\n",
    "> hey @UGentstudent! this is not a clean #nlp tweet :-D http://www.ugent.be\n",
    "\n",
    "__Step 3__ - Remove unwanted words, punctuation, URLs, whitespaces... For example, we could choose to only retain alphanumerical chatacters, but this always depends on the application at hand. In our case, we will want to keep hashtags and mentions, but remove URLs and emoticons/emojis.\n",
    "\n",
    "> hey @UGentstudent! this is not a clean #nlp tweet\n",
    "\n",
    "__Step 4__ - Splitting the text into separate words. This is a process called _tokenization_ and there exist many different methods of tokenizing a text. The most simple method finds all whitespaces and uses them to split the text. More advanced tokenizers also take into account punctuation and other textual markers.\n",
    "\n",
    "> ['hey', '@UGentstudent', '!', 'this', 'is', 'not', 'a', 'clean', '#nlp', 'tweet']\n",
    "\n",
    "__Step 5__ - Remove stop words. These are words that hardly contribute to the meaning of a text, such as \"the\", \"a\", \"an\", \"is\", \"and\", \"our\", etc. There are lists available of common stop words in English (e.g. https://gist.github.com/sebleier/554280 ). Some lists include words such as \"not\", but such words can shape the semantic meaning of a text, so always be careful which words get removed from your text.\n",
    "\n",
    "> ['hey', '@UGentstudent', '!', 'not', 'clean', '#nlp', 'tweet']\n",
    "\n",
    "__Step 6__ - In some applications (such a document classification) it can also be useful to apply _stemming_ to the text (https://en.wikipedia.org/wiki/Stemming ). Stemming essentially means that words are normalized, such that for example \"fishing\", \"fished\" and \"fisher\" all reduce to the same stem \"fish\". There exist many stemming algorithms (such as Porter stemming), but we will not cover stemming in this lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ns0YqP7Ll43"
   },
   "source": [
    "### 2.1.2 Regular expressions\n",
    "URLs appear very often in tweets and contribute almost no semantic message. It is essentially noise, and we therefore want to remove them. For this purpose, we will use regular expressions, a powerful string processing tool and syntax that is available in almost all high-level programming languages. Regular expressions (regex in short) can be very daunting to work with and can be a complete course on its own! Luckily there are useful online tools such as RegexPlanet (www.regexplanet.com) and StackOverflow that can help you in the process.\n",
    "\n",
    "As an examplary exercise, we will remove all non alphanumeric characters from our tweet using regexes. Consider the following examples, play with them, and after that, try to come up with a way of removing all non-alphanumeric characters from our tweet (but keeping the spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "BND1qQ0MLl43",
    "outputId": "b70e4284-1e75-470a-cd28-4bb88e4ec497",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(re.sub(r'de', '', 'abcde'))           #abc\n",
    "print(re.sub(r'[ae]', '', 'abcde'))         #bcd\n",
    "print(re.sub(r'[a-c]', '', 'abcde'))        #de\n",
    "print(re.sub(r'[a-bd-e]', '', 'abcde'))     #c\n",
    "print(re.sub(r'[^a-b]', '', 'abcde'))       #ab\n",
    "print(re.sub(r'a*', '', 'aaaabbc'))         #bbc\n",
    "print(re.sub(r'a?bbb', '', 'aabbbbc'))      #abc\n",
    "print(re.sub(r'a+bbb', '', 'aabbbbc'))      #bc\n",
    "print(re.sub(r'[^a-e]', '_', 'AaBbCcDdEe')) #_a_b_c_d_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "rGIxpLxbLl47",
    "outputId": "fa799f86-df5f-44bf-c07a-6ada26809dbf",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "original_tweet = \"HEY @UGentstudent! This isn't a clean #NLP tweet :-D http://www.ugent.be\"\n",
    "print(re.sub(r'[^a-zA-Z0-9\\s]', '', original_tweet))\n",
    "print(re.sub(r'[^\\w\\s]', '', original_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFjF388_Ll49"
   },
   "source": [
    "As stated above, we could teach hours on regular expressions alone. A nice layout of what regexes can do in Python, is given in the official documentation: https://docs.python.org/3/howto/regex.html. Some useful shortcuts are given there: `\\s` stands for any whitespace character, `\\d` stands for a digit, `\\w` stands for any alphanumberic character, `.` stands for any character.\n",
    "\n",
    "Here is a regular expression that can be used to remove URLs from a piece of text, and we test it on the original tweet above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "foLiM4UBLl4-",
    "outputId": "455c14c2-74b4-49e6-fd2f-9c7e8eaf1e73",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "original_tweet = \"HEY @UGentstudent! This isn't a clean #NLP tweet :-D http://www.ugent.be\"\n",
    "url_regex = r'https?://\\S+'\n",
    "# or (more complex and more general):\n",
    "# url_regex = r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*'\n",
    "\n",
    "re.sub(url_regex, '', original_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHREzE0fLl5D"
   },
   "source": [
    "The following example uses a regular expression to replace all numbers (i.e. one or more digits) with a single character '0' in a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZEhs06KELl5D",
    "outputId": "54cad7b8-a68a-44ee-84c6-5c5e0a65ba7e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "number_regex = r'\\d+'\n",
    "\n",
    "re.sub(number_regex, '0', '1, 2, 3 up to 45678.90')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkP5CPGRLl5G"
   },
   "source": [
    "Now, we find all non-alphanumeric characters in the whole dataset. For this purpose, we use the `set` datastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "uYOIMTUbLl5H",
    "outputId": "68a14712-7dd2-4308-e9d3-2fa0252d7ab7",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "punct = set()\n",
    "for t in dataset:\n",
    "    purged_tweet = re.sub(r'\\w', '', t.text).strip()\n",
    "    if len(purged_tweet) > 0:\n",
    "        for character in purged_tweet:\n",
    "            punct.add(character)\n",
    "\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mKDPrwAvLl5L"
   },
   "source": [
    "#### 2. Cleaning tweets\n",
    "We will now extend the Tweet class by writing a new function that performs all necessary text cleaning for that tweet. We will save the cleaned text as a new variable in the object.\n",
    "\n",
    "**Question:** Copy the code of the Tweet class above and write a new method `clean(self)`. This method will perform a series of cleaning operations on the tweet's text. The resulting cleaned text is saved as a new attribute `self.cleaned_text` of that class. Do the following cleaning operations in given order:\n",
    "\n",
    "1. Convert the original text to lowercase.\n",
    "2. Remove URLs.\n",
    "3. Replace all numbers by 0.\n",
    "4. Remove punctuation, but __be careful not to lose whitespaces, #, @, ' (single quote)__. For this you can use the set of non-alphanumeric characters you have found above! Since a lot of punctuation characters start with a backslash, it is easier to use the built-in `replace()` function on a Python string to remove a punctuation character.\n",
    "5. Replace all whitespace with a single space.\n",
    "6. Make sure the whitespaces at the beginning and end of the tweet are removed (look at the `strip()` function).\n",
    "\n",
    "Afterwards, import the CSV file again, and perform cleaning on all tweets. Take a look at the resulting cleaned texts and fix any mistakes: data cleaning is always an iterative process in which you improve your cleaning algorithm step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zIDLvtAsLl5L",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "punct = {'üíÅ', '[', '‚Äô', '~', 'üí™', 'üìö', 'üè°', '-', 'üê£', 'üá∫', '‚Äù', 'Ã∂', '\\u200a', ';', 'üçï', ' ', '!', '%', ',', 'üëá', '¬Æ', 'üåà', '?', 'üèΩ', '=', 'üí®', '‚úÖ', '‚úî', ')', '|', '‚Äò', '\\xa0', 'üóΩ', '&', 'üèº', '¬ø', '‚Ä¶', 'üéì', 'üëâ', '‚ùå', 'üéß', 'üëà', 'üöÇ', '+', 'ü§ñ', 'üëé', '‚Üí', '¬°', 'ü§î', 'Ô∏è', 'üë∏', '@', 'üá∏', ':', '‚Äú', '‚Ä¢', 'üèø', 'üèª', 'üëÄ', 'üëè', '‚Äî', ']', '‚úì', '\"', '\\u200b', 'üé§', '\\n', '.', '(', '$', '‚ù§', '‚¨á', '#', 'üëç', \"'\", '/', '*', 'üèæ', '‚Äì', 'üëø'}\n",
    "\n",
    "punct.remove(' ')  # keep spaces\n",
    "punct.remove('#')  # keep hashtags\n",
    "punct.remove('@')  # keep mentions\n",
    "punct.remove('\\'') # keep single quotes (in order to retain I'm, isn't, etc.)\n",
    "\n",
    "class Tweet:    \n",
    "    def __init__(self, tweet_id, handle, text, is_retweet, original_author, time, lang, retweet_count, favorite_count):\n",
    "        self.tweet_id = tweet_id\n",
    "        self.handle = handle\n",
    "        self.text = text\n",
    "        self.is_retweet = is_retweet == 'True'\n",
    "        self.original_author = original_author\n",
    "        self.time = time\n",
    "        self.lang = lang\n",
    "        self.retweet_count = int(retweet_count)\n",
    "        self.favorite_count = int(favorite_count)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.tweet_id + ' / ' + self.handle + \": \" + self.text\n",
    "    \n",
    "    def clean(self):\n",
    "        pass\n",
    "        ## Answer: add function to clean tweets:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kxLhT4TxLl5O",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with open('tweets_stripped.csv', 'r') as data_file:\n",
    "    datareader = csv.reader(data_file, delimiter=',', quotechar='\"')\n",
    "    for i, row in enumerate(datareader):\n",
    "        if i > 0:\n",
    "            t = Tweet(row[0], row[1], row[2], row[3], row[4], row[5], row[10], row[11], row[12])\n",
    "            t.clean()\n",
    "            dataset.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7zaXwwbWLl5R",
    "outputId": "9f592e58-d681-4627-d4a5-db6e55ec25cc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for t in dataset[:10]:\n",
    "    print(t.cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNXrHfwULl5U"
   },
   "source": [
    "#### 3. Tokenization\n",
    "Tokenization is the process of splitting a text into a sequence of separate words. It is an essential step in NLP, since words are the most basic building blocks that provide a meaning or sentiment to any text. Natural language models often therefore often work with words as input features (although in some models the characters itself are used instead of words).\n",
    "\n",
    "The most basic type of tokenization is to split the text whenever a whitespace occurs. Take a look at the `split()` function and use it to tokenize the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "PlzDSrEMLl5V",
    "outputId": "80048a34-2fce-4509-bc58-01ff3879321f",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tweet_text = \"couldn't be more proud of @hillaryclinton her vision and command during last night's debate showed that she's ready to be our next @potus\"\n",
    "\n",
    "tweet_tokens = tweet_text.split(\" \")\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvfKmedrLl5X"
   },
   "source": [
    "It works pretty well already, but some tokens such as _\"couldn't\"_ are ideally expanded to _\"could\"_ and _\"not\"_. NLTK is Python's Natural Language ToolKit that contains many methods and algorithms to tokenize a text. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Gs1HyJE0Ll5Y",
    "outputId": "08623730-7212-4ac4-aa1b-064131976724",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet_tokens = word_tokenize(tweet_text)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IDieRulHLl5b"
   },
   "source": [
    "**Question:** Take a look at the official NLTK documentation (https://www.nltk.org/api/nltk.tokenize.html) and you will notice that it contains a specific Twitter-aware tokenizer. Use it to tokenize the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "F4cQecKgLl5b",
    "outputId": "1dd1d54c-5923-462a-a50d-10e7429ca14e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "## Answer: tokenize tweet with TweetTokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQg50DEzLl5e"
   },
   "source": [
    "One reason to use Python for machine learning (and NLP) is that thousands of fellow computer scientists and researchers are using it as well for that purpose. Chances are that whatever problem you are trying to tackle, someone has done it already. Regarding tweet tokenization, there are numerous GitHub projects to be found that have solved this problem, some better or more user-friendly than others. We have had pretty good experience with the following GitHub repository: https://github.com/erikavaris/tokenizer.\n",
    "\n",
    "**Question:** Take a look at the install instructions, install the package, and after that use the package to tokenize the tweet. Also, the package allows for text normalization, thereby expanding _\"couldn't\"_ into _\"could\"_ and _\"not\"_. You can find out how by taking a look at the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "colab_type": "code",
    "id": "WDBI6qikLl5e",
    "outputId": "335aecee-c83d-408f-c07a-c345e0e1b87c",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPnoOi84Ll5h"
   },
   "source": [
    "Keep in mind that tokenization is never perfect, and should be tuned to your application at hand!\n",
    "\n",
    "**Question:** Now, again, copy the Tweet class in the cell below and write a `tokenize(self)` function. Store the resulting tokens in the `self.tokens` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "p1sA2iKNLl5i",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer: extend Tweet class with tokenize function:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_R-XATSLl5m"
   },
   "source": [
    "Now, test the tokenization on the tweet dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "vKOMj4JhLl5n",
    "outputId": "f079e2cb-4af8-4546-8793-028564796713",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with open('tweets_stripped.csv', 'r') as data_file:\n",
    "    datareader = csv.reader(data_file, delimiter=',', quotechar='\"')\n",
    "    for i, row in enumerate(datareader):\n",
    "        if i > 0:\n",
    "            t = Tweet(row[0], row[1], row[2], row[3], row[4], row[5], row[10], row[11], row[12])\n",
    "            t.clean()\n",
    "            t.tokenize()\n",
    "            dataset.append(t)\n",
    "            \n",
    "            \n",
    "for t in dataset[:3]:\n",
    "    print(t.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubXhx3VwLl5q"
   },
   "source": [
    "#### 4. Building a vocabulary\n",
    "For machine learning models it is important that datapoints have a numerical representation. Many machine learning models are vector-based, and each class of data has is assigned to its own dimension in these vectors. What this means for us now, is that each word should be mapped to its own unique positive integer value. For example, consider a dictionary of the entire English language, then each word can be mapped to the position of that word in the dictionary. The sentence _\"proud of hillary\"_ can then be represented as a sequence of indices, e.g. [243, 5, 8723]. In other words, the text is transformed into a mathematical representation, which opens the gate to all sorts of powerful machine learning methods and models. Such models will be explored in the next lab session, which is why we cover vocabulary building here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhG5-WKRLl5r"
   },
   "source": [
    "We will now build a vocabulary for the Twitter dataset. This vocabulary will be used throughout the remainder of the lab session.\n",
    "\n",
    "**Question:** Use the `set` datastructure to find all unique words in the dataset, thereby using the tokenized tweets. How big is the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8MSKBKL3Ll5s",
    "outputId": "07bd7a76-2503-4130-92e5-fca7790df2f6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7uqfHC9Ll5v"
   },
   "source": [
    "**Question:** Now create a dictionary `word_to_ix` which maps a word to a unique index (from 0 up to the length of the vocabulary). Also, create a dictionary `ix_to_word` which does the opposite mapping. Check if you can translate a word to an index, and use that index to find the original word again. What is the theoretical time complexity of a dictionary lookup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "3bWRE8dnLl5x",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer part 1: create word_to_ix and ix_to_word\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Io38ZZs_Ll5z",
    "outputId": "8e868fe4-d102-467d-d77b-cf1e21b8f531",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer part 2: test these on some words (e.g., 'hillary', ...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pw_YMY_uLl51"
   },
   "source": [
    "Some additional questions and exercises for which you can use the precalculated tokens or the vocabulary. Feel free to play around with the data as you like.\n",
    "\n",
    "\n",
    "**Question 1.** Which words are used most by Trump and Hillary? Create a bar chart for each candidate to illustrate this.\n",
    "\n",
    "**Question 2.** Which Twitter accounts are mentioned most by Trump and Hillary?\n",
    "\n",
    "*__Tip__: to get better results for the first question, take a look at NLTK on how to __ignore common English stopwords__.*\n",
    "\n",
    "*__Tip__: for question Q2, try out the use of `defaultdict`, a dictionary for which new keys are automatically assigned some default value (here: int 0)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "wqRNtGqYLl52",
    "outputId": "8f50659b-7f34-478a-d2ce-65b447b684f8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer to Q1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 747
    },
    "colab_type": "code",
    "id": "jygasPXZLl55",
    "outputId": "ec48a93b-d54a-4ee4-fa57-1f262a3d23ad",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer to Q2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xq4X9XKkLl57"
   },
   "source": [
    "At this point, we are ready to explore more advanced predictive analytics on texts. Therefore, make sure to be up to speed with all text preprocessing concepts that were covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7CkEUivLl58"
   },
   "source": [
    "Rule-based systems are among the very first AI systems. They can execute a task by following a set of rules that were predetermined by a human. For example: a very simple rule for a basic thermostat would be to start heating when the temperature drops below 20 degrees, and to switch off when the temperature rises above 22 degrees. Rule-based systems can be powerful, but are not flexible, can become really complex and it is very time consuming to determine a proper set of rules.\n",
    "\n",
    "Nevertheless, it is always a good exercise to create and experiment with a rule-based AI system. As a matter of exercise, we will build a simple classifier that will predict whether a tweet comes from Donald Trump or from Hillary Clinton based on the principle of **majority voting**. For this, use the following steps. Feel free to tweak the rules of the game!\n",
    "\n",
    "**Question 3.** For all words in the dataset, count how many times Trump and Hillary use that word. E.g. Trump uses the word 'great' 5623 times, and Hillary only 820 times. It gives you the quantities $N_{H,t}$ and $N_{T,t}$ for resp. the number of times Hillary and Trump use token $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_XMdJC33Ll58",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer to Question 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCrtba0dLl5-"
   },
   "source": [
    "**Question 4.** Now take a new tweet (not in the dataset) and for every word (token) in that tweet, perform majority voting. If $N_{H,t} > N_{T,t}$, a vote is given to Hillary for that tweet. Otherwise, Trump gets a vote.\n",
    "Whoever gets the most votes for the given tweet, is the winner, and is therefore also the predicted author of the tweet.\n",
    "Test this on the tweet given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "mt3LMXeeLl5_",
    "outputId": "1e690c83-0950-4cb4-947b-c31cfe7d2265",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tweet_to_classify = \"make america great again\"\n",
    "\n",
    "## Answer to Question 4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USGOlNpnLl6B"
   },
   "source": [
    "\n",
    "# Part 2 - Sentiment Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSREMHFyLl6C"
   },
   "source": [
    "## Machine Learning in Python\n",
    "\n",
    "\n",
    "<center> \n",
    "<img src=\"img/scikit-learn-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/numpy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/scipy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"http://localhost:8888/static/base/images/logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/matplotlib-logo.png\" style=\"max-width: 120px; display: inline\"/>\n",
    "<img src=\"img/pandas-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "</center>\n",
    "\n",
    "### Scikit-Learn\n",
    "\n",
    "* Machine learning library written in __Python__\n",
    "* __Simple and efficient__, for both experts and non-experts\n",
    "* Classical, __well-established machine learning algorithms__\n",
    "* Shipped with <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> and <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "b9_Xt18xLl6D",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# if necessary, install the needed packages by uncommenting the following lines:\n",
    "# ! conda install numpy scipy scikit-learn jupyter matplotlib  -y\n",
    "# ! conda install python-graphviz -y\n",
    "# ! pip install pydotplus\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Optimized operations on arrays and lists\n",
    "import numpy as np\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "# General machine Learning\n",
    "import sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqYNt9XZOf8D"
   },
   "source": [
    "---\n",
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "navrIRLmLl6I"
   },
   "source": [
    "## IMDB Movie Reviews\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/1sgprf9bmsva487/bowman_imdb.PNG?dl=1\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgD7tEyvLl6I"
   },
   "source": [
    "**Question:**\n",
    "- Load in the IMDB reviews stored as csv file `data/train.csv` using `pandas.read_csv()`\n",
    "- Print the column headers of the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "Zzdz61ZbOiA5",
    "outputId": "c5ccf13b-c8c6-47f7-d4b9-7f168f682686",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RngKS6m4Ll6L"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IQq_PxPzLl6L"
   },
   "source": [
    "**Questions:**\n",
    "- How many reviews are included in the collection?\n",
    "- How many different tokens are used in the reviews? \n",
    "- What is the average rating of all the reviews?\n",
    "- What is the average length of a review?\n",
    "\n",
    "Note: alle text has been tokenized and seperated using white space, thus, calling `.split()` on text is sufficient for tokenization in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "YjNz3NxVLl6M",
    "outputId": "44d0c3d9-bf9a-4f84-f8e8-8330c517e6e8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9W-WkS_Ll6O"
   },
   "source": [
    "We can now take a look at the rating distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "iAWtmcUSLl6P",
    "outputId": "37bd9cf8-e108-4d4e-dfe9-7b536c9953cb",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(data['rating'], 5, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "RWwMIW2wLl6Q",
    "outputId": "e427f16c-921d-4ec2-cc8d-150c2434e3f5",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(len([len(x.split()) for x in data['text']]))\n",
    "sns.distplot([len(x.split()) for x in data['text']], range(0,50,1), kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_oXT9UcyLl6S"
   },
   "source": [
    "**Question:**\n",
    "- How is the vocabulary distributed? (Make a histogram of the word counts.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "p0hS-NOHLl6T",
    "outputId": "271ddbe8-08ce-4bd6-e7f7-973070130666",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3fPUsjBLl6W"
   },
   "source": [
    "As an illustration, below is code to visual the word frequencies in a word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "colab_type": "code",
    "id": "0d3YxYsmLl6X",
    "outputId": "b0cda733-3642-4927-8f83-6007e85e3954",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Illustration: plot frequencies in a word cloud\n",
    "\n",
    "# if necessary, install the needed packages by uncommenting the following line:\n",
    "! pip install wordcloud\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Store text from pandas data frame in list\n",
    "text = ' '.join(data['text'][data['label']==1]) # join all reviews in training set\n",
    "print('Example text:\\n', data['text'][0])\n",
    "\n",
    "# limit word count\n",
    "wordcount = 1000\n",
    "# stop words\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"br\")\n",
    "print('\\nEnglish stopwords:\\n', ', '.join(sorted(list(stopwords))))\n",
    "\n",
    "# setup word cloud\n",
    "wc = WordCloud(scale=3, background_color=\"white\", max_words=wordcount, stopwords=stopwords, width=800, height=600)\n",
    "\n",
    "# generate word cloud\n",
    "wc.generate(text)\n",
    "\n",
    "# show\n",
    "print('\\nWordcloud:')\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfIjHXQBN_LS"
   },
   "source": [
    "---\n",
    "# Supervised Machine Learning\n",
    "\n",
    "Data comes as a finite learning set ${\\cal L} = (X, y)$ where\n",
    "* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n",
    "* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$.\n",
    "\n",
    "The goal of supervised classification is to build a function $f: {\\cal X} \\mapsto {\\cal Y}$ minimizing the expected error over the training data:\n",
    "\n",
    "$$\n",
    "Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$.\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Diagnosing disease from symptoms;\n",
    "- Recognising cats in pictures;\n",
    "- Identifying body parts with Kinect cameras;\n",
    "- ...\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D58nNXWPJtwU"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxzjPG5hLl6a"
   },
   "source": [
    "Now that we have the data, we need to build some sort of **feature representation** of our data. One of the simplest things we can do is to represent each sentence as a bag of its words. As part of determining what constitutes a work (or \"token\"), we'll have to choose how to tokenize the data. Let's do the simplest thing for now and just split on whitespace. More sophisticated methods might use a tokenizer from an outside library, such as NLTK or SpaCy.\n",
    "\n",
    "\n",
    "The most intuitive way to do so is to use a **Bags of Words** representation using using [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/kn9t26hmth9ir18/jakevdp_features.png?dl=1\">\n",
    "\n",
    "Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "For each document #i, count the number of occurrences of each word w and store it in X[i, j] (row i and column j in the feature matrix X) as the value of feature #j where j is the index of word w in the dictionary.\n",
    "The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "\n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a few thousand distinct words will be used. For this reason we say that bags of words are typically high-dimensional **sparse** datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "scipy.sparse matrices are data structures that do exactly this, and scikit-learn has built-in support for these structures\n",
    "\n",
    "Below, we \n",
    "- Import a `CountVectorizer` from the `sklearn.feature_extraction.text` package. \n",
    "- Create a `CountVectorizer` object named `tokenizer`\n",
    "\n",
    "[See documentation for parameters of this object](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "o4MtdRqfJtwV",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# creating a tokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "tokenizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bO3MhrBJtwW"
   },
   "source": [
    "# <span style=\"color:blue\">Exercise 1 </span>\n",
    "\n",
    "- The `fit()` method of the vectorizer object learns a vocabulary dictionary of all tokens in the raw documents. Fit the tokenizer using `tokenizer.fit()`. Explore the tokenizer object (e.g. `dir(tokenizer)`) and the fit function (e.g., `help(tokenizer.fit)`)\n",
    "- Transform text from the training collection using `tokenizer.transform(data['text'])` into a vector named `X`\n",
    "- Print the dimensions of the feature-matrix, `X`\n",
    "- Print the tokenizer dictionary using `tokenizer.vocabulary_`\n",
    "- Show the feature representation of the first document: what features are non-zero?\n",
    "\n",
    "- Finetune the tokenizer:\n",
    " - Specify the tokenizer to strip accents.\n",
    " - Create a tokenizer that learns unigrams as well as bigrams, fit the data, and print the number of features.\n",
    " - Reduce the tokenizer to remove stopwords, only retain tokens that occur 2 or more times, and print the number of different words\n",
    " - Reduce to only the 5.000 most often occurring tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "NvUU7BeOLl6c",
    "outputId": "d92630bd-8102-4ea0-c6cf-1e917fd0cf3e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer to exercise 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72TA8AINLl6e"
   },
   "source": [
    "---\n",
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLi1tEoNLl6g"
   },
   "source": [
    "The goal of supervised classification is to build a function $f: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n",
    "\n",
    "$$\n",
    "Err(f) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, f(X)) \\}\n",
    "$$\n",
    "\n",
    "where $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kp6a0EGZLl6g"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvmpU72eLl6h"
   },
   "source": [
    "**Decision Trees** are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a token occurs in a review or not), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "[A visual introduction to decision trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/yoq17ki09oy6nfq/jakevdp_decision_tree.png?dl=1\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "daJUsMnlLl6i"
   },
   "source": [
    "- Import the `DecisionTreeClassifier` from the `sklearn.tree` module. [See documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- Create a `DecisionTreeClassifier` object named `dt_clf`; limit the number of leaf nodes to 5 and use information gain as criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mIIVAv1sLl6i",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou7dmBKcLl6l"
   },
   "source": [
    "In order to learn the \"best\" parameters for our model based on the training data, use scikit-learn‚Äôs `dt_clf.fit(features, vectors)` method. Inside this method, the parameters are according to some loss function (see slides).\n",
    "\n",
    "- Store sentiment labels in a `y` variable\n",
    "- Fit the classifier object calling the `fit()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "6p5P3zCTLl6l",
    "outputId": "91a3923b-eb84-438e-e6eb-47bd9263a060",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "colab_type": "code",
    "id": "zDrvgXTFLl6o",
    "outputId": "5fc275bd-a97e-4432-d138-0c8ec9000e4c",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(dt_clf, \n",
    "                           feature_names=list(tokenizer.get_feature_names()),\n",
    "                           filled=True, \n",
    "                           rounded=True,\n",
    "                           special_characters=True, class_names=['Negative','Positive'])\n",
    "graph = pydotplus.graphviz.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_Jb6aX4Ll6q"
   },
   "source": [
    "What feature is tested first? Does this decision correspond with your intuition? \n",
    "What about the 2nd test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mb9T7YebLl6q"
   },
   "source": [
    "**<< Insert your answer here in MarkDown >>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCn-8jmuLl6r"
   },
   "source": [
    "## Naive Bayes Classification\n",
    "\n",
    "__Naive Bayes classifiers__ are built on Bayesian classification methods.\n",
    "These rely on Bayes's theorem, which is an equation describing the relationship of __conditional probabilities__ of statistical quantities.\n",
    "In Bayesian classification, we're interested in finding the probability of a label $y$ given some observed features, which we can write as $P(y~|~{\\rm x})$.\n",
    "Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
    "\n",
    "$$\n",
    "P(y~|~{\\rm x}) = \\frac{P({\\rm x}~|~y)P(y)}{P({\\rm x})}\n",
    "$$\n",
    "\n",
    "If we are trying to decide between negative and positive‚Äîthen one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\n",
    "\\underset{y \\in \\{negative, positive\\}}{\\text{argmax}} P(y|x)\n",
    "$$\n",
    "\n",
    "All we need now is some model by which we can compute $P({\\rm x}~|~y)$ for each label.\n",
    "Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
    "Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
    "The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label (i.e., the independence of co-occuring words), we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
    "Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trFiK-pzLl6r"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uc-KYcrALl6s"
   },
   "source": [
    "__Logistic regression__ models the probability $P(y|x;w)$ that a review, $x$, falls into a specific category $y \\in \\{0,1\\}$ (negative, positive). Using a vector of weights, w. \n",
    "\n",
    "If $P(y=1|x;w)$ > 0.5 then class positive is selected\n",
    "\n",
    "- For Bag-of-Words vector\n",
    "$$x = [x_1,x_2,...,x_{|V|}], x \\in \\mathbb{R}^{|V|} $$\n",
    "\n",
    "- Parameter vector (One per class)\n",
    "$$w = [w_1,w_2,...,w_{|V|}], w \\in \\mathbb{R}^{|V|} $$\n",
    "\n",
    "- Linear scoring function $f$\n",
    "$$f_w{(x)} = w_1x_1+w_2x_2+...+w_{|V|}x_{|V|} = w^Tx \\in \\mathbb{R}$$\n",
    "\n",
    "- Simgoid Function\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}} \\in [0,1]$$\n",
    "\n",
    "As $z$ goes from $-\\infty$ to $\\infty$, $\\sigma(z)$ goes from 0 to 1\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/tt0q4ktc4na12yr/sigmoid.png?dl=1\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDy3ngpFLl6t"
   },
   "source": [
    "$$P(y=\\text{positive}|x;w) = \\sigma(f_w(x)) $$\n",
    "\n",
    "$$  = \\frac{1}{1+e^{-(w^Tx)}}$$\n",
    "\n",
    "\n",
    "<!-- <img src=\"img/lr.png\" width=300px> -->\n",
    "\n",
    "$$log P(y=\\text{positive}|x;w)= - log(1+e^{{-(w^Tx)}}) $$\n",
    "\n",
    "\n",
    "**Learning** is formulated as the maximizing the likelihood of the correct answer or minimizing the negative log of the likelihood (probability) of the correct answer\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "$$ \\underset{\\text{w}\\in R^{|V|}}{min}\\sum_{i}^{N} log(1+e^{{-(w^Tx)}}) $$\n",
    "\n",
    "if $\\sigma(f(x))$ > 0.5 then class y = positive is selected\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYh9OyiRLl6t"
   },
   "source": [
    "# <span style=\"color:blue\">Exercise 2 </span>\n",
    "\n",
    "- Import the `BernoulliNB` from the `sklearn.naive_bayes` module. [See documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "- Create a new BernoulliNB object named `nb_clf`\n",
    "- Fit the model on the data\n",
    "- Which are the most significant tokens for both classes? One way to measure the importance of tokens to compute the ratio of the posterior probabilities for each label. Print the feature-weights and feature names learned by the classifier using `tokenizer.get_feature_names()` and `nb_clf.feature_log_prob_`. \n",
    "\n",
    "\n",
    "$$\\frac{P(y = \\text{positive}~|~{\\rm x})}{P(y = \\text{negative}~|~{\\rm x})}$$\n",
    "\n",
    "$$ log(P(y = \\text{positive}~|~{\\rm x})) - log(P(y = \\text{negative}~|~{\\rm x})) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "colab_type": "code",
    "id": "PwrDR6CvJtwf",
    "outputId": "2ea6e18d-9404-404a-e729-3cea662db1b0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuQ1Xcw3Ll6v"
   },
   "source": [
    "- Import the `LogisticRegression` from the `sklearn.linear_model` module. [See documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- Create a new LogisticRegression object named `lr_clf`\n",
    "- Fit the model on the data\n",
    "- Print the feature-weights, $w$, and tokens learned by the classifier using `tokenizer.get_feature_names()` and `lr_clf.coef_`. Which are the most important ones for both classes? Weights provide insights into important features, which tokens are weighted strongly for both classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "colab_type": "code",
    "id": "JlYEKPzLLl6w",
    "outputId": "9f92beb9-d011-42de-f062-5a8c4aa62442",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eTFcHlPJtwj"
   },
   "source": [
    "We now have a trained sentiment analysis model.\n",
    "\n",
    "In the next section we'll evaluate our models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxDzwyjYJtwj"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sK-XUmozLl6z"
   },
   "source": [
    "Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, this can be done using the predict() method. For the sake of this example, our \"new data\" will be a grid of x values, and we will ask what y values the model predicts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GyL683gJtwj"
   },
   "source": [
    "How well does our model do? Let's define a function to see our model's accuracy on some data split and see how well we fit the training data. We'll make use of the `clf.predict()` interface for generating predictions.\n",
    "\n",
    "\n",
    "### Metrics: Accuracy, Precision, Recall, F$_1$\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/xhjwoqd6tnxxaeb/bowman_metrics.png?dl=1\" width=800px>\n",
    "\n",
    "$$F = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n",
    "\n",
    "- Import all the metrics mentioned above from the `sklearn.metrics` module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lR6gfS3XLl60",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer: import metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VveGLnI5Ll63"
   },
   "source": [
    "\n",
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-kIG8d6Ll63"
   },
   "source": [
    "\n",
    "- Evaluate accuracy by a classifier on the same data it was trained on using `clf.predict()` and `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "5pT_BhZHJtwl",
    "outputId": "a4d9467d-d144-4cc8-e6a0-45475e1015b4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQDFzp44Ll65"
   },
   "source": [
    "Issue: the training error is a __biased__ estimate of the generalization error.\n",
    "\n",
    "Solution: Divide ${\\cal L}$ into disjoint parts called training and test sets (a common choice is using 80% for training and 20% for test).\n",
    "- Use the training set for fitting the model;\n",
    "- Use the test set for evaluation only, thereby yielding an unbiased estimate.\n",
    "\n",
    "This could be done by hand, but it is more convenient to use the `train_test_split` utility function from `sklearn.model_selection`.\n",
    "\n",
    "- import the `train_test_split` of the `sklearn.model_selection` module\n",
    "- Create a `X_train`, `X_test`, `y_train`, `y_test` from the data collection \n",
    "- Print the shapes of the new arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "hnEljpp1Ll66",
    "outputId": "590ec625-f5a7-4261-bd2f-6836b915fff0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ahdgOewJtwn"
   },
   "source": [
    "\n",
    "- How well do we do on held-out data? Evaluate accuracy by a Logistic regression classifier on the test data using `clf.predict(X_test)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "vV-e_V7hJtwo",
    "outputId": "dc70ce96-4f63-4664-fd6f-31e861f2d311",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moJjL1bwJtwq"
   },
   "source": [
    "We see a big drop, ~20 accuracy, on held-out data, so we overfit the training data. We can go back and revise our approach (e.g. by playing around with the different parameters for the [logistic regression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) and re-fitting on the training data, and then see how well we do on the held-out validation data.\n",
    "\n",
    "By doing this, however, we'll be fitting to the validation data. At some point, we'll want to evaluate one completely new data. Which is what the test split is for. The test split should be used as sparingly as possible!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "3ez_fJFnLl6-",
    "outputId": "1f33b4eb-9a84-4799-b3db-33ccc725dfbb",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_val, clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsARbX0zLl6_"
   },
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a-vDD2i1Ll7A"
   },
   "source": [
    "When models have enough flexibility to nearly perfectly account for the fine features in the data, they \n",
    "can learn to very accurately describe the training data. The model's precise form can become more reflective of the particular noise properties of the training data (which will be different in held-out data) than the intrinsic properties of whatever process generated that data (i.e., the actual 'signal' in the data, which will be present in held-out data as well).\n",
    "\n",
    "Such a model is said to __*overfit*__ the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high __*variance*__, and it may poorly perform on unseen data.\n",
    "\n",
    "A model is said to __*underfit*__ the data when it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high __*bias*__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZa-GpvpLl7A"
   },
   "source": [
    "Overfitting can be countered in many ways.  In this tutorial we focus on the technique of __*regularization*__ of a logistic regression classifier.\n",
    "Learning in Logistic Regression is formulated as the optimization of\n",
    "\n",
    "$$ \\underbrace{\\underset{\\text{w}\\in R^{|V|}}{min}\\sum_{i}^{N} log(1+e^{{-(\\sigma(w^Tx))}})}_{Loss}$$\n",
    "\n",
    "When a linear model overfits, weights tend to become very large. One way to counter this, is to penalize large weights by adding an additional component to the loss function called a regularization.\n",
    "\n",
    "$$ \\underbrace{\\underset{\\text{w}\\in R^{|V|}}{min}\\sum_{i}^{N} log(1+e^{{-(\\sigma(w^Tx))}})}_{Loss} + \\underbrace{\\frac{1}{C} ||w||^2}_{Regularization}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- What is the effect of the `C` parameter on the generalization performance? Evaluate in terms of accuracy, as well as the distribution (e.g., make a histogram) of the weight coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "BTa7u3yQLl7B",
    "outputId": "8275ecff-7eb9-4e22-9480-286037874cb3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_88foISOLl7C"
   },
   "source": [
    "\n",
    "## The Bias-Variance trade-off\n",
    "\n",
    "Fundamentally, the question of \"the best model\" is about finding a sweet spot in the tradeoff between *bias* and *variance*.\n",
    "\n",
    "\n",
    "- __Under-fitting, High Bias__: the model is too simple and does not capture the true relation between X and Y.\n",
    "- __Over-fitting, High Variance__: the model is too specific to the training set and does not generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "colab_type": "code",
    "id": "MlfiLM4xLl7D",
    "outputId": "59301bcd-f05c-490c-bae1-2fb034732723",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Evaluate parameter range in CV\n",
    "param_range = [0.001,0.01,0.1,0.4,0.6,1,1.2,1.4,1.6,2]\n",
    "param_name = \"C\"\n",
    "clf = LogisticRegression()\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    clf, X, y, \n",
    "    param_name=param_name, \n",
    "    param_range=param_range, cv=2, n_jobs=-1, scoring=\"accuracy\")\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot parameter VS estimated error\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.xlabel(param_name)\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlim(min(param_range), max(param_range))\n",
    "plt.plot(param_range, train_scores_mean, color=\"blue\", label=\"Training acc.\")\n",
    "plt.fill_between(param_range, \n",
    "                 train_scores_mean + train_scores_std,\n",
    "                 train_scores_mean - train_scores_std,\n",
    "                 alpha=0.2, color=\"blue\")\n",
    "plt.plot(param_range, test_scores_mean, color=\"red\", label=\"CV acc.\")\n",
    "plt.fill_between(param_range, \n",
    "                 test_scores_mean + test_scores_std,\n",
    "                 test_scores_mean - test_scores_std, \n",
    "                 alpha=0.2, color=\"red\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f224OK3_Ll7E"
   },
   "source": [
    "<table>\n",
    "<tr><td><p style=\"clear: both;\"> <img src=\"https://www.dropbox.com/s/mpmrstr7k5z0lku/jakevdp_biasvariance.png?dl=1\" width=400px></td><td> <img src=\"https://www.dropbox.com/s/4dyuvb3b6swwcgy/louppe_bv.png?dl=1\" width=400px> <td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O62PYz8qLl7E"
   },
   "source": [
    "## Cross Validation\n",
    "\n",
    "When evaluating different settings (___hyperparameters___) for estimators, such as the $C$ setting that must be manually set for a logistic regression, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can ___leak___ into the model and evaluation metrics no longer report on generalization performance. To solve this problem, the data not used for training is split into a ___validation set___ and the actual ___test set___. Training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called __cross-validation__ (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into $k$ smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k ‚Äúfolds‚Äù:\n",
    "\n",
    "A model is trained using all but one of the folds as training data;\n",
    "the resulting model is evaluated on the remaining part of the data (i.e., it is used as a validation set to compute a performance measure such as accuracy).\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/132oue6yj29cml3/jakevdp_cross.png?dl=1\" width=600px>\n",
    "\n",
    "- What is the average accuracy over five folds of training, for a logistic regression classifier with default parameters (in sklearn)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TdU-1yAnLl7F",
    "outputId": "a70ef86f-b488-4036-e1aa-758d76486225",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VzArIYTZLl7G"
   },
   "source": [
    "<img src=\"https://www.dropbox.com/s/6qqluvighbqub27/epfl_traintest.PNG?dl=1\" width=600px>\n",
    "\n",
    "__Summary__: Beware of bias when you estimate model performance:\n",
    "* Training score is often an optimistic estimate of the true performance;\n",
    "* __The same data should not be used both for training and evaluation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBZm7D0vJtwt"
   },
   "source": [
    "# <span style=\"color:blue\">Exercise 3</span> \n",
    "\n",
    "\n",
    "\n",
    "- Experiment with other measures for regularization of the models  discussed during the session\n",
    "    - What is $l1$ and $l2$ regularization and what is the effect on the weights when changing the `penalty` parameter of the LogisticRegression object to `l1`?\n",
    "    - What is the effect of the `max_features` parameter of the tokenizer?\n",
    "- Which of the models performs best on the validation data? Discover and import other models from the sklearn package ([KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier), [Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), ...)\n",
    "- Can you think of ways to combine predictions by different classifiers? \n",
    "- A held-out collection of reviews is stored in `data/test.csv`. In the remaining time, try to maximize your model's performance on the CV splits without evaluating on it (until the end of class). How you go about that is completely open (feature engineering, modeling, optimization, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "8_bLY7Q4Jtww",
    "outputId": "97b8b1dc-6e29-4e2a-fa40-2c1d0a3ab1bc",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJArpwFXLl7J"
   },
   "source": [
    "## Bonus: Kaggle Competition\n",
    "- Congratulations! You have made your first sentiment classifier and predicted sentiment for a held-out collection of movie reviews. The ground truth for the held-out data is stored on the online data science platform Kaggle. \n",
    "- What is the performance of your model? How do you rank among your peers?\n",
    "- Now try to improve your model by adding new models, vectorizers, additional data,... (talk to us or go looking online to get ideas)\n",
    "- Think of ways to include knowledge from external sources in your model. \n",
    "- Submit your predictions to the in-class [Kaggle Competition](https://www.kaggle.com/c/ugentnlp-sentiment). This competition is private and will be removed. We will keep it open up until the deadline for submitting this notebook (i.e., next Sunday at 11:59PM). \n",
    "- Feel free to comment on any successful submissions, or provide insights into which methods worked better than others. However, this remains optional.\n",
    "\n",
    "__Keep in mind, the limit of submissions is set at 20 per day!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "T4kmb6SyLl7K",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1WTtRS8Ll7M"
   },
   "source": [
    "## Tutorial based on\n",
    "\n",
    "- [NYU Course - Introduction to Natural Language Understanding - Samuel Bowman](https://docs.google.com/document/d/1kXhxA4iit2fhAJJGOb32bb151cKLJtW8xWuyMVwqD6s/edit)\n",
    "- [Introduction to Scikit Learn - Gilles Louppe](https://github.com/glouppe/tutorials-scikit-learn)\n",
    "- [EPFL Introduction to Deep Learning - Fran√ßois Fleuret](https://fleuret.org/ee559/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HHJrNVLWLl42",
    "uCn-8jmuLl6r",
    "x1WTtRS8Ll7M"
   ],
   "name": "NLP_Lab1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
