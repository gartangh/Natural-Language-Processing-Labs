{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP_lab2_student.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8XQeIF6rBI6"
   },
   "source": [
    "# Lab session 2: Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qGoVr7SrBI8"
   },
   "source": [
    "This lab covers classical and neural language models as seen in the theory lectures. \n",
    "\n",
    "General instructions:\n",
    "- Complete the code where needed\n",
    "- Provide answers to questions only in the cell where indicated\n",
    "- **Do not alter the evaluation cells** (`## evaluation`) in any way as they are needed for the partly automated evaluation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ppr_iMCj2bi7"
   },
   "source": [
    "## **How AI can write a paper!**\n",
    "\n",
    "We shall train our language model on a corpora of scientific articles and see if we can generate a new one!\n",
    "\n",
    "<img src=\"https://media1.tenor.com/images/073dfe5d68e2490903aa51ae0ac633de/tenor.gif?itemid=3536848\" alt=\"img\" width=\"512px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "92oj5l862bi-",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "# import necessary packages\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "VLOk8it2UPwy",
    "pycharm": {
     "is_executing": false
    },
    "colab": {}
   },
   "source": [
    "# for reproducability\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cFtNG-Q2bjJ"
   },
   "source": [
    "### **Data exploration**\n",
    "\n",
    "Lets download and look into the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "T7oF8ejH_t2k",
    "outputId": "c1adcce0-4c6f-411f-ab85-d04e40e056b7",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    }
   },
   "source": [
    "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "!tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2020-03-17 18:40:00--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
      "--2020-03-17 18:40:00--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com/cd/0/get/A0HdI_Tz83vOs8JjUXLF1bxP79ykjHxTHhPWIsuaz2iiuWzw76BsR3XjsgLL9_kudfKpnDpNF9JbWP0w22l-bO7FF-gJlRXQUNnhXJr6eQ0xlQ/file?dl=1# [following]\n",
      "--2020-03-17 18:40:01--  https://uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com/cd/0/get/A0HdI_Tz83vOs8JjUXLF1bxP79ykjHxTHhPWIsuaz2iiuWzw76BsR3XjsgLL9_kudfKpnDpNF9JbWP0w22l-bO7FF-gJlRXQUNnhXJr6eQ0xlQ/file?dl=1\n",
      "Resolving uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com (uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n",
      "Connecting to uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com (uc5a6eb7fa5a70a1548b52b84503.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18933283 (18M) [application/binary]\n",
      "Saving to: ‘arxivData.json.tar.gz’\n",
      "\n",
      "arxivData.json.tar. 100%[===================>]  18.06M  16.1MB/s    in 1.1s    \n",
      "\n",
      "2020-03-17 18:40:03 (16.1 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
      "\n",
      "arxivData.json\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>[{'name': 'A. N. Gorban'}, {'name': 'A. Y. Zin...</td>\n",
       "      <td>2</td>\n",
       "      <td>0809.0490v2</td>\n",
       "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
       "      <td>9</td>\n",
       "      <td>In many physical, statistical, biological and ...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Principal Graphs and Manifolds</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>[{'name': 'Behnam Neyshabur'}, {'name': 'Ryota...</td>\n",
       "      <td>27</td>\n",
       "      <td>1503.00036v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>We investigate the capacity, convexity and cha...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Norm-Based Capacity Control in Neural Networks</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22961</th>\n",
       "      <td>[{'name': 'Maria De-Arteaga'}, {'name': 'Willi...</td>\n",
       "      <td>27</td>\n",
       "      <td>1711.09522v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>11</td>\n",
       "      <td>This is the Proceedings of NIPS 2017 Workshop ...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Proceedings of NIPS 2017 Workshop on Machine L...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19448</th>\n",
       "      <td>[{'name': 'J. Keppens'}, {'name': 'Q. Shen'}]</td>\n",
       "      <td>30</td>\n",
       "      <td>1107.0035v1</td>\n",
       "      <td>[{'rel': 'related', 'href': 'http://dx.doi.org...</td>\n",
       "      <td>6</td>\n",
       "      <td>The predominant knowledge-based approach to au...</td>\n",
       "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Compositional Model Repositories via Dynamic C...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>[{'name': 'Kevin T. Kelly'}, {'name': 'Conor M...</td>\n",
       "      <td>15</td>\n",
       "      <td>1203.3488v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>Over the past two decades, several consistent ...</td>\n",
       "      <td>[{'term': 'cs.LG', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Causal Conclusions that Flip Repeatedly and Th...</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  author  ...  year\n",
       "857    [{'name': 'A. N. Gorban'}, {'name': 'A. Y. Zin...  ...  2008\n",
       "144    [{'name': 'Behnam Neyshabur'}, {'name': 'Ryota...  ...  2015\n",
       "22961  [{'name': 'Maria De-Arteaga'}, {'name': 'Willi...  ...  2017\n",
       "19448      [{'name': 'J. Keppens'}, {'name': 'Q. Shen'}]  ...  2011\n",
       "2331   [{'name': 'Kevin T. Kelly'}, {'name': 'Conor M...  ...  2012\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "YY8udsOVSh_F",
    "outputId": "804c05b1-2121-499d-a22d-d288d87e3219",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    }
   },
   "source": [
    "# Assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
    "\n",
    "# Sample the first 3 lines...\n",
    "sorted(lines, key=len)[:3]"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "tbXJxSil2bjM",
    "outputId": "cddee92b-66d7-4195-c7d2-3565cb24e52d",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    }
   },
   "source": [
    "# Convert lines into strings of space-separated tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "lines = [tknzr.tokenize(sent.lower()) for sent in lines]\n",
    "lines = [' '.join(sent) for sent in lines]\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['differential contrastive divergence ; this paper has been retracted .',\n",
       " 'what does artificial life tell us about death ? ; short philosophical essay',\n",
       " 'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6DRT4A8q2bjP"
   },
   "source": [
    "### **N-Gram Language Model**\n",
    "\n",
    "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "It can do so by following the chain rule:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
    "\n",
    "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
    "\n",
    "One popular approximation is to assume that the next word only depends on a finite amount of previous words:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
    "\n",
    "Such a model is called an __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on the 2 previous words. \n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "You can also sometimes see that approximation under the name of the _n-th order markov assumption_.\n",
    "\n",
    "The first stage in building such a model is counting all word occurences given the $n-1$ previous words:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "nWJiczkr2bjR",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# special tokens: \n",
    "# - unk represents absent tokens, \n",
    "# - eos is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines, n):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = defaultdict(Counter)\n",
    "      \n",
    "    # counts[(word 1, word 2 ... word n-1)][word n] \n",
    "    #    = how many times word n occurred after (word 1 ... word n-1)\n",
    "    \n",
    "    for sent in lines:\n",
    "        sent = sent.split() + [EOS]\n",
    "        prefix = [UNK] * (n-1)\n",
    "        for word in sent:\n",
    "            ############### for student ################\n",
    "            counts[tuple(prefix)][word] +=1\n",
    "            if len(prefix) > 0:\n",
    "              prefix.pop(0)\n",
    "              prefix.append(word)\n",
    "            ############################################\n",
    "\n",
    "    return counts"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "vQJIOfRf2bjU",
    "outputId": "3d827d8d-6bca-41c4-e882-9720aaa51786",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "\n",
    "print('well done!')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "well done!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1PER9Ue2bjX"
   },
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.\n",
    "The simplest way to compute probabilities is in proporiton to counts:\n",
    "\n",
    "$$ P(w | \\textit{prefix}) = { \\textit{Count}(\\textit{prefix}, w) \\over \\sum_{w' \\in \\textit{Vocab}} \\textit{Count}(\\textit{prefix}, w') } $$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "LGj4zemN2bjY",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w | prefix) given n-gram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.probs = defaultdict(Counter)\n",
    "        \n",
    "        # compute token probabilities (self.probs), given the counts computed above\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        ############### for student ################\n",
    "        for sent in lines:\n",
    "          sent = sent.split() + [EOS]\n",
    "          prefix = [UNK] * (n-1)\n",
    "          for word in sent:\n",
    "            sum = 0\n",
    "            for count in counts[tuple(prefix)].values():\n",
    "              sum += count\n",
    "            self.probs[tuple(prefix)][word] = counts[tuple(prefix)][word] / sum\n",
    "            if len(prefix) > 0:\n",
    "              prefix.pop(0)\n",
    "              prefix.append(word)\n",
    "        ############################################\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : its probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfbC9yas2bjb"
   },
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "47vBFod12bjc",
    "outputId": "967be334-a067-48a3-d5a2-0775f2d5d588",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\"\n",
    "\n",
    "print(\"Good job!\")"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6DqTkI32bjf"
   },
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-c5_g1km2bjg",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PSUlkZLx2bjj"
   },
   "source": [
    "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add the next token by sampling from the probabilities over the vocabulary at each point in the sequence.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Instead of sampling with probabilities, one can also take the most likely token, sample from among the top-K most likely tokens, or sample with a certain *temperature*. In the latter case (temperature), one samples from\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{w'} P(w' | X) ^ {1 / \\tau}}$$\n",
    "\n",
    "Where $\\tau > 0$ is the model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish. For sampling from a given probability distribution, the function `np.random.choice` can be used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "B_bUavwG2bjj",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "def get_next_token(lm, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "\n",
    "    ############### for student ################ \n",
    "    if temperature == 0:\n",
    "      inverse_temperature = 1\n",
    "    else:\n",
    "      inverse_temperature = 1 / temperature\n",
    "\n",
    "    possible_next_tokens = lm.get_possible_next_tokens(prefix)\n",
    "    probs_dict = {}\n",
    "    sum = 0\n",
    "    for possible_next_token in possible_next_tokens:\n",
    "      next_token_prob = lm.get_next_token_prob(prefix, possible_next_token)\n",
    "      next_token_prob_with_temperature = pow(next_token_prob, inverse_temperature)\n",
    "      probs_dict[possible_next_token] = next_token_prob_with_temperature\n",
    "      sum += next_token_prob_with_temperature\n",
    "\n",
    "\n",
    "    keys =  list(probs_dict.keys())\n",
    "    values = [value / sum for value in list(probs_dict.values())]\n",
    "\n",
    "    if temperature == 0:\n",
    "      max_indices = np.where(values == np.amax(values))[0]\n",
    "      token = keys[np.random.choice(max_indices)]\n",
    "    else:\n",
    "      token = np.random.choice(keys, p=values)\n",
    "    ############################################\n",
    "\n",
    "    return token           \n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "rR7huhTz2bjm",
    "outputId": "73f342cc-b068-4bb4-dc84-2680c148ea11",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "from collections import Counter\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "\n",
    "print(\"Looks nice!\")"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4putrZEE2bjo"
   },
   "source": [
    "Let's have fun with this model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HpZkqe6B2bjp",
    "outputId": "1405c047-d235-4b8c-9ca8-cf52451bcd63",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "prefix = 'artificial' # <- your ideas on the start of your AI generated scientific paper :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "artificial persuasion in abstract argumentation frameworks . this work , we use a technique to solve this problem has been successful in extracting organization names in various real - world data regression problems ; to solve the problem defaults were intended to cause parameters to be adopted , where there are still subject to some state - observation model by allowing nonmonotonic inferences and show distribution - independent behaviors . we evaluate the performance of each position . over - segmentation to assign dependency relation matrix , and can solve various fault detection problems arising in random experiments . _EOS_\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "haAMgd8E2bjr",
    "outputId": "2b7273fd-53ad-4951-c308-3efa22a54655",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.4)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "bridging the gap between the amount of data mining , and it is possible to train a deep neural networks ( cnns ) and the results of a set of labeled data for the task of learning the parameters of the proposed method is significantly faster than the original data set . we show that the proposed method . _EOS_\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nTsCEeCrBJv"
   },
   "source": [
    "### Question:\n",
    "1. How does the temperature parameter affect the generated samples?\n",
    "\n",
    "When the temperature is very small, more likely tokens will be sampled with an even higher probability.\n",
    "There will be a lower chance of getting 'exotic' words and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSFf9Xxn2bju"
   },
   "source": [
    "### **Evaluating language models: perplexity**\n",
    "\n",
    "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "On the corpus level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
    "\n",
    "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "18C1wbQ_2bjv",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprob, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    N = 0  # number of tokens\n",
    "    ppl = 0.0 # perplexity\n",
    "    \n",
    "    # https://stats.stackexchange.com/questions/129352/how-to-find-the-perplexity-of-a-corpus\n",
    "    \n",
    "    ############### for student ################\n",
    "    sum = 0\n",
    "    for sent in lines:\n",
    "      # calculate sentence probability\n",
    "      sent_log_prob = 0\n",
    "      sent = sent.split() + [EOS]\n",
    "      prefix = [UNK] * (lm.n-1)\n",
    "      for word in sent:\n",
    "        N += 1\n",
    "        word_prob = lm.get_next_token_prob(\" \".join(prefix), word)\n",
    "        if word_prob == 0:\n",
    "          sent_log_prob += min_logprob\n",
    "        else:\n",
    "          sent_log_prob += max(min_logprob, np.log(word_prob))\n",
    "        if len(prefix) > 0:\n",
    "          prefix.pop(0)\n",
    "          prefix.append(word)\n",
    "\n",
    "      # update sum with sentence log probability or\n",
    "      sum += sent_log_prob\n",
    "\n",
    "    # calculate perplexity\n",
    "    ppl = np.exp(-1/N * sum)\n",
    "    ############################################\n",
    "    \n",
    "    return ppl"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Qd_S3CgK2bjy",
    "outputId": "3d82f0f6-d45c-4721-ece7-44fff6d2c642",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N-gram models should overfit and have lower ppl\"\n",
    "\n",
    "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))\n",
    "\n",
    "print('Well done!')"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n",
      "Well done!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPvh4rw_2bj0"
   },
   "source": [
    "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "qN0hUDIn2bj1",
    "outputId": "60a443f4-8a32-4fbc-9f04-e32742e91c99",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    }
   },
   "source": [
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=SEED)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-67404a333378>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mlm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNGramLanguageModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_lines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mppx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mperplexity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_lines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"N = %i, Perplexity = %.5f\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mppx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-1e860a1c1a4e>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, lines, n)\u001B[0m\n\u001B[1;32m     23\u001B[0m             \u001B[0msum\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m               \u001B[0msum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEPXFNpVrBJ3"
   },
   "source": [
    "### Question:\n",
    "\n",
    "2. Do you expect increasing/decreasing perplexities for language models with longer n-grams (i.e., higher values of n)? Does this correspond with the test output you observe above? If not: can you explain this?\n",
    "\n",
    "The longer the n-gram, the lower the perplexity on the train set.\n",
    "When n becomes to big, too many sequences of words have nog been seen yet, so the results will get worse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsAKFXD8yCO3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  LM Smoothing\n",
    "\n",
    "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
    "To battle this issue, there's a technique called __smoothing__. \n",
    "\n",
    "The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is _additive smoothing_ (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w | \\textit{prefix}) = \\frac{\\textit{Count}(\\textit{prefix}, w) \\color{red}{+ \\delta}}{\\sum_{w' \\in \\textit{Vocab}}(\\textit{Count}(\\textit{prefix}, w') \\color{red}{+ \\delta})} $$\n",
    "\n",
    "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution.\n",
    "\n",
    "Update `self.probs` inside the `__init__` to handle smoothing. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "O83GCDSbMfSv",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n) # \n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts) #\n",
    "        self.probs = defaultdict(Counter)\n",
    "        \n",
    "        # compute token proabilities\n",
    "        # probs[prefix][token] = ...\n",
    "        ############### for student ################\n",
    "        for sent in lines:\n",
    "          sent = sent.split() + [EOS]\n",
    "          prefix = [UNK] * (n-1)\n",
    "          for word in sent:\n",
    "            sum = 0\n",
    "            for count in counts[tuple(prefix)].values():\n",
    "              sum += count\n",
    "            self.probs[tuple(prefix)][word] = (counts[tuple(prefix)][word] + delta) / (sum + len(self.vocab)*delta)\n",
    "            if len(prefix) > 0:\n",
    "              prefix.pop(0)\n",
    "              prefix.append(word)\n",
    "        ############################################\n",
    "\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "        \n",
    "\n",
    "\n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        "
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "i267TTABNMN9",
    "outputId": "585ec7c9-ef92-4c9d-b0d4-88980662c6a4",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "# test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"You broke something ! :)\"\n",
    "\n",
    "print('Great!')"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Great!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Jk3hCsLiEJEU",
    "outputId": "7c03af95-8d66-44aa-8c24-ea691c5627e2",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    }
   },
   "source": [
    "# calculate perplexity for LaplaceLanguageModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=SEED)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-14bad2f987f5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mn\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0mlm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLaplaceLanguageModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlines\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_lines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mppx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mperplexity\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_lines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"N = %i, Perplexity = %.5f\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mppx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-18-d5e179700809>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, lines, n, delta)\u001B[0m\n\u001B[1;32m     15\u001B[0m             \u001B[0msum\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcounts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m               \u001B[0msum\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mcount\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprobs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mcounts\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdelta\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0msum\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mdelta\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefix\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lS9tYi3rBKA"
   },
   "source": [
    "### Question:\n",
    "3.   In a bigram language model (without smoothing), which of the following two phrases do you expect to have higher probablity? Why?\n",
    " - *and and*\n",
    " - *this paper* \n",
    "\n",
    "The probability of \"and and\" is expected to be zero, as it is not part of the valid english grammar.\n",
    "The sequence \"this paper\" can appear in the corpus, so the probability is expected to be higher than 0, and thus also expected to higher than that of \"and and\".\n",
    "\n",
    " 4.   If we add smoothing, how would the probability relation change for the above phrases? \n",
    "\n",
    "The probability of \"and and\" will be higher than 0, but still lower than that of \"this paper\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AWJ09IbrBKB"
   },
   "source": [
    "Train both language models (smoothing and non-smoothing version) on `dummy_lines` and report perplexity for the given phrases."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "eWYPIVS8rBKC",
    "outputId": "0dfb938d-6e8f-464f-ed99-049f86d7e7cd",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    }
   },
   "source": [
    "lm_names = [\"without smoothing\", \"with smoothing\"]\n",
    "phrases = [[\"and and\"], [\"this paper\"]]\n",
    "\n",
    "############### for student ################\n",
    "ngram = NGramLanguageModel(n=2, lines=dummy_lines)\n",
    "laplace = LaplaceLanguageModel(n=2, lines=dummy_lines)\n",
    "models = [ngram, laplace]\n",
    "\n",
    "for phrase in phrases:\n",
    "  for model, lm_name in zip(models, lm_names):\n",
    "      ppl = perplexity(model, phrase)\n",
    "      ############################################\n",
    "      print(\"%s: phrase = '%s' --> pp = %.2E\" % (lm_name, phrase, ppl))\n",
    "        "
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "without smoothing: phrase = '['and and']' --> pp = 1.00E+50\n",
      "with smoothing: phrase = '['and and']' --> pp = 1.01E+03\n",
      "without smoothing: phrase = '['this paper']' --> pp = 2.79E+33\n",
      "with smoothing: phrase = '['this paper']' --> pp = 3.48E+02\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWVsI9znVJgE"
   },
   "source": [
    "## **Deep Learning Based Language Models**\n",
    "\n",
    "We've checked out statistical approaches to language models so far. Now let's go find out what deep learning has to offer. We're gonna use the same dataset as before. \n",
    "\n",
    "\n",
    "![alt text](https://vipulvaibhaw.files.wordpress.com/2019/04/saltbae_pytorch.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qDQAvQiNELH8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are going to implement the simplest recurrent neural network (RNN) known as the Elman RNN. Its hidden state aims to encapsulate the information for all previous input elements in order to help the network to take into account *the past*. Since there is no hidden state during the first step, we feed the network with an initial state of zero values (or randomly initilized values). Next, we feed it the first token $\\textit{A}$ together with the hidden state of the previous step, to predcit the next output ($\\textit{girl}$). We'll repeat this procudure until the end of sequence. \n",
    "\n",
    "We can summarize the above explanation into a simple equation as:\n",
    "\n",
    "$$h_t = F(x_{t}, h_{t-1}) = f(W_{x}x_t + W_{h}h_{t-1} + b),$$\n",
    "\n",
    "where $x_{t}$ is input, $h_{t}$ is hidden state, $b$ is bias term and $f$ is the `tanh` non-linearity.\n",
    "\n",
    "<a href=\"https://ibb.co/TcWR5CQ\"><img src=\"https://adventuresinmachinelearning.com/wp-content/uploads/2017/09/Recurrent-neural-network.png\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "CEcJFRkIrBKF",
    "outputId": "82ce9c91-611b-4faa-d219-491bb48cd8b1",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "### If you don't have pytorch yet: install it in the current kernel first.\n",
    "### Uncomment next 2 lines to do that.\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} pytorch\n",
    "\n",
    "# first import necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for reproducibility \n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.enabled = False \n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n",
      "cuda\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4_fr0ct2Mm3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Tokenization**\n",
    "\n",
    "Before implementing the neural network itself, lets prepare the data. We need special tokens:\n",
    "\n",
    "* Begin Of Sequence  (__BOS__) - this token is at the start of each sequence. We use it so that we always have non-empty input to our neural network. $P(x_t) = P(x_1 | BOS)$\n",
    "* End Of Sequence (__EOS__) - you guess it... this token is at the end of each sequence. The catch is that it should __not__ occur anywhere else except at the very end. If our model produces this token, the sequence is over.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "5YSv47V12LDg",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "BOS, EOS = '<s>', '</s>'\n",
    "text = [BOS + ' ' + line + ' ' + EOS for line in lines] # concatenate BOS and EOS to all sentences\n",
    "text = [line.split() for line in text]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "zXpDD07qrBKN",
    "outputId": "75b7840b-f1fb-4d59-de7d-cc083b57a46d",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    }
   },
   "source": [
    "# let's print the first sentence \n",
    "print(text[0])"
   ],
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "['<s>', 'dual', 'recurrent', 'attention', 'units', 'for', 'visual', 'question', 'answering', ';', 'we', 'propose', 'an', 'architecture', 'for', 'vqa', 'which', 'utilizes', 'recurrent', 'layers', 'to', 'generate', 'visual', 'and', 'textual', 'attention', '.', 'the', 'memory', 'characteristic', 'of', 'the', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'a', 'rich', 'joint', 'embedding', 'of', 'visual', 'and', 'textual', 'features', 'and', 'enables', 'the', 'model', 'to', 'reason', 'relations', 'between', 'several', 'parts', 'of', 'the', 'image', 'and', 'question', '.', 'our', 'single', 'model', 'outperforms', 'the', 'first', 'place', 'winner', 'on', 'the', 'vqa', '1', '.', '0', 'dataset', ',', 'performs', 'within', 'margin', 'to', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'ensemble', 'model', '.', 'we', 'also', 'experiment', 'with', 'replacing', 'attention', 'mechanisms', 'in', 'other', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'with', 'our', 'implementation', 'and', 'show', 'increased', 'accuracy', '.', 'in', 'both', 'cases', ',', 'our', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'in', 'tasks', 'requiring', 'sequential', 'or', 'relational', 'reasoning', 'on', 'the', 'vqa', 'dataset', '.', '</s>']\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5jp_3uQ_iHT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us convert our raw text into sequences of ids. First, create two sorted dictionaries and map each token to an id. Second, create the target data sequences based on the input. Note that the target is the same as the input, except that it is one token ahead of the input."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "8KWb98fYCLJ0",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_data_lm(data):\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    for sent in data:\n",
    "        word_counts.update(sent)\n",
    "\n",
    "    sorted_token = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # create two dictionaries to convert token to id or vice-verca\n",
    "    id_to_token = {k: w for k, w in enumerate(sorted_token)}\n",
    "    token_to_id = {w: k for k, w in id_to_token.items()}\n",
    "    \n",
    "    n_token = len(id_to_token)\n",
    "    \n",
    "    tokenized_text = [[token_to_id[w] for w in sent] for sent in data]\n",
    "    \n",
    "    # output is one token ahead\n",
    "    inp_text = [sent[:-1] for sent in tokenized_text]\n",
    "    out_text = [sent[1: ] for sent in tokenized_text]\n",
    "\n",
    "    return id_to_token, token_to_id, n_token, inp_text, out_text"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "zzdcPMY1L3nb",
    "outputId": "4cde35d3-c9aa-45f1-e124-876cddc41449",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    }
   },
   "source": [
    "id_to_token, token_to_id, n_token, inp_text, out_text = get_data_lm(text[0:1])\n",
    "\n",
    "print(\"-- raw text:\", text[0:1])\n",
    "print('-' * 100)\n",
    "print(\"-- input text:\", inp_text)\n",
    "print('-' * 100)\n",
    "print(\"-- output text:\", out_text)"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "-- raw text: [['<s>', 'dual', 'recurrent', 'attention', 'units', 'for', 'visual', 'question', 'answering', ';', 'we', 'propose', 'an', 'architecture', 'for', 'vqa', 'which', 'utilizes', 'recurrent', 'layers', 'to', 'generate', 'visual', 'and', 'textual', 'attention', '.', 'the', 'memory', 'characteristic', 'of', 'the', 'proposed', 'recurrent', 'attention', 'units', 'offers', 'a', 'rich', 'joint', 'embedding', 'of', 'visual', 'and', 'textual', 'features', 'and', 'enables', 'the', 'model', 'to', 'reason', 'relations', 'between', 'several', 'parts', 'of', 'the', 'image', 'and', 'question', '.', 'our', 'single', 'model', 'outperforms', 'the', 'first', 'place', 'winner', 'on', 'the', 'vqa', '1', '.', '0', 'dataset', ',', 'performs', 'within', 'margin', 'to', 'the', 'current', 'state', '-', 'of', '-', 'the', '-', 'art', 'ensemble', 'model', '.', 'we', 'also', 'experiment', 'with', 'replacing', 'attention', 'mechanisms', 'in', 'other', 'state', '-', 'of', '-', 'the', '-', 'art', 'models', 'with', 'our', 'implementation', 'and', 'show', 'increased', 'accuracy', '.', 'in', 'both', 'cases', ',', 'our', 'recurrent', 'attention', 'mechanism', 'improves', 'performance', 'in', 'tasks', 'requiring', 'sequential', 'or', 'relational', 'reasoning', 'on', 'the', 'vqa', 'dataset', '.', '</s>']]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-- input text: [[24, 25, 6, 3, 13, 14, 7, 15, 26, 27, 16, 28, 29, 30, 14, 8, 31, 32, 6, 33, 9, 34, 7, 4, 17, 3, 1, 0, 35, 36, 5, 0, 37, 6, 3, 13, 38, 39, 40, 41, 42, 5, 7, 4, 17, 43, 4, 44, 0, 10, 9, 45, 46, 47, 48, 49, 5, 0, 50, 4, 15, 1, 11, 51, 10, 52, 0, 53, 54, 55, 18, 0, 8, 56, 1, 57, 19, 20, 58, 59, 60, 9, 0, 61, 21, 2, 5, 2, 0, 2, 22, 62, 10, 1, 16, 63, 64, 23, 65, 3, 66, 12, 67, 21, 2, 5, 2, 0, 2, 22, 68, 23, 11, 69, 4, 70, 71, 72, 1, 12, 73, 74, 20, 11, 6, 3, 75, 76, 77, 12, 78, 79, 80, 81, 82, 83, 18, 0, 8, 19, 1]]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "-- output text: [[25, 6, 3, 13, 14, 7, 15, 26, 27, 16, 28, 29, 30, 14, 8, 31, 32, 6, 33, 9, 34, 7, 4, 17, 3, 1, 0, 35, 36, 5, 0, 37, 6, 3, 13, 38, 39, 40, 41, 42, 5, 7, 4, 17, 43, 4, 44, 0, 10, 9, 45, 46, 47, 48, 49, 5, 0, 50, 4, 15, 1, 11, 51, 10, 52, 0, 53, 54, 55, 18, 0, 8, 56, 1, 57, 19, 20, 58, 59, 60, 9, 0, 61, 21, 2, 5, 2, 0, 2, 22, 62, 10, 1, 16, 63, 64, 23, 65, 3, 66, 12, 67, 21, 2, 5, 2, 0, 2, 22, 68, 23, 11, 69, 4, 70, 71, 72, 1, 12, 73, 74, 20, 11, 6, 3, 75, 76, 77, 12, 78, 79, 80, 81, 82, 83, 18, 0, 8, 19, 1, 84]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XxSBfxkGpj1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For training, we won't put the entire sequence through the model at once. As explained in the theory lectures, we will limit the sequence length over which we apply back-propagation (which we'll call the *bptt*, or the back-propagation-through-time length), and arrange the bptt-long segments into mini-batches for parallel training (also see the slides, for considerations on choosing the mini-batch size).\n",
    "\n",
    "Let us first convert the sequences into **mini-batches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjZR3aS1Gen-"
   },
   "source": [
    "Suppose our batch size is 4 (for benefiting from a gpu, you'll need to scale this up), bptt is 3 (in practice it will be much longer, though), and our data consists of a 1-dimensional tensor containing 36 token id's. Each batch will contain a 4x3 input tensor and a 4x3 target tensor, except for the last batch (we can discard the last one during training). As you already know, the target batch is one token ahead (in terms of the original sequence) of the input batch since our task is language modeling! The input/output tensor for the first batch will be something like the following:\n",
    "\n",
    "**Input batch**:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/522/1*DVCsHtcfX8Hrb-1BJg92fw.png)\n",
    "\n",
    "**Target batch**:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/504/1*gczU2zRHXnQ0SgXLanIK9g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nxnxl7IIrBKX"
   },
   "source": [
    "We'll convert the input (`inp_text`) and target (`out_text`) indices from the previous steps into mini-batches as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "a4flBYwz3M3t",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "from itertools import chain \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def get_batches(inp_text, out_text, batch_size, seq_size):\n",
    "    \n",
    "    # shuffle the sentences\n",
    "    itext, otext = shuffle(inp_text, out_text)\n",
    "    \n",
    "    # flatten the data\n",
    "    itext = list(chain(*itext))\n",
    "    otext = list(chain(*otext))\n",
    "    \n",
    "    # work out how cleanly we can divide the dataset into batch_size parts.\n",
    "    num_batches = int(len(itext) / (seq_size * batch_size))\n",
    "    \n",
    "    # trim off any extra elements that wouldn't cleanly fit\n",
    "    itext = itext[:num_batches * batch_size * seq_size] \n",
    "    otext = otext[:num_batches * batch_size * seq_size]\n",
    "\n",
    "    itext = np.reshape(itext, (batch_size, -1)) # batch_size * tokens\n",
    "    otext = np.reshape(otext, (batch_size, -1)) # batch_size * tokens\n",
    "    \n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield itext[:, i:i + seq_size], otext[:, i:i + seq_size]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNfRxe7CrBKa"
   },
   "source": [
    "### Question:\n",
    "5.   Does trimming, in above, reduce capabilites of the train model?\n",
    "\n",
    "NO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "TAX_L9btrBKb",
    "outputId": "6f8f74a1-49a9-4718-b72d-5faf319b0275",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    }
   },
   "source": [
    "bs = 4\n",
    "bptt = 10\n",
    "batches = get_batches(inp_text, out_text, bs, bptt)\n",
    "\n",
    "for x,y in batches:\n",
    "    print('input:', x.shape)\n",
    "    print(x)\n",
    "    print('-' * 35)\n",
    "    print('target:', y.shape)\n",
    "    print(y)\n",
    "    break;"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "input: (4, 10)\n",
      "[[24 25  6  3 13 14  7 15 26 27]\n",
      " [ 5  0 37  6  3 13 38 39 40 41]\n",
      " [15  1 11 51 10 52  0 53 54 55]\n",
      " [22 62 10  1 16 63 64 23 65  3]]\n",
      "-----------------------------------\n",
      "target: (4, 10)\n",
      "[[25  6  3 13 14  7 15 26 27 16]\n",
      " [ 0 37  6  3 13 38 39 40 41 42]\n",
      " [ 1 11 51 10 52  0 53 54 55 18]\n",
      " [62 10  1 16 63 64 23 65  3 66]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a3wrsxdHpsO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Let's build the model**\n",
    "\n",
    "By extending the `nn.Module` you can easily develop your own recurrent cell in pytorch. In this part we shall implement **Elman** Recurrent Network. This module receives a sequence of feature vectors and returns two tensors. Please study the nodes as defined in `__init__` and initialized using `init_weights`, and correctly fill in the `forward` method in line with the formula for the Elman RNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "1gVBw_VvFo27",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz):\n",
    "        super(RNN, self).__init__()\n",
    "        # necessary parameters in Elman RNN\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        \n",
    "        self.fc_x = nn.Linear(self.input_sz, self.hidden_sz, bias=False)\n",
    "        self.fc_h = nn.Linear(self.hidden_sz, self.hidden_sz) #default: bias = True\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc_x.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_h.weight)\n",
    "        self.fc_h.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        \"\"\"\n",
    "        :param x: batch of sequences of input symbols (represented as vectors)\n",
    "                  dimensions of x will be batch_size * sequence_length * input_sz\n",
    "        :param state: state vector, of size self.hidden_sz\n",
    "        :return: hidden_seq, state; where state is final output state,\n",
    "                 hidden_seq is list of hidden states h_t (see fig. above)\n",
    "        \"\"\"\n",
    "        \n",
    "        # things to do:\n",
    "        # 1) if state is None, initialize it with zeros (use `torch.zeros`)\n",
    "        # 2) iterate over time, each time applying the RNN formula, and\n",
    "        #    concatenate the state tensors to hidden_seq (with `torch.cat`)\n",
    "        # 3) reshape hidden_seq from (seq, batch, feature) to (batch, seq, feature), with `Tensor.transpose`\n",
    "        \n",
    "        hidden_seq = []\n",
    "        \n",
    "        ############### for student ################\n",
    "        if state == None:\n",
    "          state = torch.zeros((x.size(0),self.hidden_sz)).to(device)\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        time_steps = x.size(0)\n",
    "        for t in range(time_steps):\n",
    "            state = torch.tanh(self.fc_x(x[t]) + self.fc_h(state))\n",
    "            hidden_seq.append(state)\n",
    "        ############################################\n",
    "\n",
    "        out = torch.stack(hidden_seq)\n",
    "        out = out.permute(1,0,2)\n",
    "        \n",
    "        return out, state"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HiVk59-DSaTi",
    "outputId": "719f3666-202f-49e4-d576-2f72ea235dc1",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "# Let's test the RNN module\n",
    "arr = torch.rand([1, 2, 2]).to(device) # tensor dimension: batch size x bptt x features\n",
    "my_rnn = RNN(2, 2).to(device)\n",
    "out, state = my_rnn(arr)\n",
    "\n",
    "assert out.shape == torch.Size([1, 2, 2]), out.shape\n",
    "assert state.shape == torch.Size([1, 2]), state.shape\n",
    "\n",
    "print(\"RNNCell completed!\")"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "RNNCell completed!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ldp30LUYPFT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The recurrent module is only one part of the neural language model, we still need an embedding layer to convert our tokens into a feature vector and a decoding layer to predict subsequent tokens. To this end, we defined a wrapper and put everything in it. We provided the necessary modules you'll need, please complete the `forward` function. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "tFiwFmga4hEA",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, n_tokens, hidden_size):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoder = nn.Embedding(n_tokens, hidden_size)  \n",
    "        self.rnn = RNN(hidden_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, n_tokens)\n",
    "    \n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        :return: logits, state; where logits = output of the decoder,\n",
    "                 and state = the final rnn state.\n",
    "        \"\"\"\n",
    "        # call the Embedding, RNN and linear decoder layer in the forward pass\n",
    "        ############### for student ################\n",
    "        embeddings = self.encoder(x)\n",
    "        out, state = self.rnn(embeddings)\n",
    "        logits = self.decoder(out)\n",
    "\n",
    "        return logits, state\n",
    "        ############################################        \n",
    "\n",
    "    def zero_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "uJ3NOjK7kftX",
    "outputId": "af044984-1157-4c45-d842-d32768d00f74",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "# Let's test the whole model\n",
    "arr = torch.randint(n_token, [64, 32], dtype=torch.long).to(device)\n",
    "\n",
    "rnn_language_model = RNNLanguageModel(n_token, 256).to(device)\n",
    "state = rnn_language_model.zero_state(64).to(device)\n",
    "out, state = rnn_language_model(arr, state)\n",
    "\n",
    "assert type(out) != type(None), 'Do you return output?'\n",
    "assert type(state) != type(None), 'Do you return state?'\n",
    "assert out.shape == torch.Size([64, 32, n_token]), out.shape\n",
    "assert state.shape == torch.Size([64, 256]), state.shape \n",
    "print(\"Sounds good! The model is complete!\")"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Sounds good! The model is complete!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ekvi0-dsHie1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Sampling**\n",
    "\n",
    "You will need a function to generate text. For your convenience, we implemented it for you. The idea is to feed one token at a time to the model and concatenate the model's output token to previously predicted tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HIzIA8JUrBKq",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "def sample(preds, n_token, temperature):\n",
    "    if temperature == 0:\n",
    "        choice = np.argmax(preds[0].tolist())\n",
    "    else:\n",
    "        preds = preds.squeeze() / temperature\n",
    "        exp_preds = np.exp(preds.tolist())\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        choice = np.random.choice([*range(n_token)], p=preds)\n",
    "    return choice"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "D8MVYJ_LVPPl",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {}
   },
   "source": [
    "def generate_text(device, net, n_token, token_to_id, id_to_token, temperature=1.0):\n",
    "    # we are in evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # initialize state\n",
    "    state_h = net.zero_state(1).to(device)\n",
    "\n",
    "    # manually feed some tokens\n",
    "    initial_words = ['recurrent', 'neural']\n",
    "    for w in initial_words:\n",
    "        ix = torch.tensor([[token_to_id[w]]], dtype=torch.long).to(device)\n",
    "        preds, state_h = net(ix, state_h)\n",
    "    \n",
    "    choice = sample(preds, n_token, temperature)\n",
    "    initial_words.append(id_to_token[choice])\n",
    "\n",
    "    # generate next tokens (50 tokens at most!)\n",
    "    for _ in range(50):\n",
    "        ix = torch.tensor([[choice]], dtype=torch.long).to(device)\n",
    "        preds, state_h = net(ix, state_h)\n",
    "        choice = sample(preds, n_token, temperature)\n",
    "        \n",
    "#         # you can stop generation \n",
    "#         if id_to_token[choice] == EOS:\n",
    "#             break;\n",
    "        \n",
    "        initial_words.append(id_to_token[choice])\n",
    "\n",
    "\n",
    "    return ' '.join(initial_words)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oh9MrQinIxWp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Training loop**\n",
    "\n",
    "A typical set of steps for training in Pytorch is:\n",
    "\n",
    "* set model in 'train' mode *(note: it will only inform the inner mechanism that we are about to train, but not actually execute the training; we still need to do that ourselves)*\n",
    "* Reset all gradients\n",
    "* Compute output, loss value, accuracy, etc\n",
    "* Perform back-propagation\n",
    "* Update the network’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "gUxKy9PzrBKx",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "seq_size = 32\n",
    "batch_size = 16\n",
    "hidden_size = 256\n",
    "temperature = 1.0\n",
    "\n",
    "dummy_text = text[0:100]\n",
    "id_to_token, token_to_id, n_token, inp_text, out_text = get_data_lm(dummy_text)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "tvxqiXVZtYE_",
    "outputId": "c06f1a24-b3d9-41ef-ffa6-b98b52cb0bf1",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "net = RNNLanguageModel(n_token, hidden_size)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "iteration = 0\n",
    "total_epochs = 100\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "for e in range(total_epochs):\n",
    "    \n",
    "    batches = get_batches(inp_text, out_text, batch_size, seq_size)\n",
    "    \n",
    "    state_h = net.zero_state(batch_size)\n",
    "    state_h = state_h.to(device)\n",
    "    \n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        # Things to do:\n",
    "        # - put model in `train` mode\n",
    "        # - set gradients to zero\n",
    "        # - model prediction for given input\n",
    "        # - calculate loss for a mini-batch\n",
    "        # - compute gradient\n",
    "        # - detach state representation by `detach()` (If we did not detach the history of hidden states \n",
    "        #   the back-propagated gradients would flow from the loss towards the beginning)\n",
    "        # - clip gradients (optional)\n",
    "        # - update parameters \n",
    "        \n",
    "        ############### for student ################\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        prediction, state_h = net(x, state_h)\n",
    "        prediction = prediction.permute(0,2,1)\n",
    "        loss = criterion(prediction, y)\n",
    "        loss.backward()\n",
    "        state_h.detach()\n",
    "        optimizer.step()\n",
    "        ############################################\n",
    "\n",
    "        if iteration % 50 == 0:\n",
    "            print('epoch: {}/{} iteration: {} loss: {}'.format(e, total_epochs, iteration, loss.item()))\n",
    "            \n",
    "        if iteration % 250 == 0:\n",
    "            print('-' * 50)\n",
    "            print(generate_text(device, net, n_token, token_to_id, id_to_token, temperature))\n",
    "            print('-' * 50)\n",
    "    \n",
    "    loss_history.append(loss.item())"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "epoch: 1/100 iteration: 50 loss: 6.073951721191406\n",
      "epoch: 2/100 iteration: 100 loss: 5.4057159423828125\n",
      "epoch: 4/100 iteration: 150 loss: 4.830307960510254\n",
      "epoch: 5/100 iteration: 200 loss: 4.531630992889404\n",
      "epoch: 6/100 iteration: 250 loss: 4.179226398468018\n",
      "--------------------------------------------------\n",
      "recurrent neural networks ? ), dependence corresponding engineering dnns reading insights insurance adoption platforms leverage free 2009 limiting knowledge shows second phi specifications obtains its saved dialogue response generation have been issues scholarly augments visualizing object memory implemented reliable forcing correlate only scratch implemented ones the current service dialogues barely utterances acquire reduce\n",
      "--------------------------------------------------\n",
      "epoch: 8/100 iteration: 300 loss: 3.6311163902282715\n",
      "epoch: 9/100 iteration: 350 loss: 3.5736477375030518\n",
      "epoch: 11/100 iteration: 400 loss: 3.075451374053955\n",
      "epoch: 12/100 iteration: 450 loss: 2.7306365966796875\n",
      "epoch: 13/100 iteration: 500 loss: 2.6814098358154297\n",
      "--------------------------------------------------\n",
      "recurrent neural networks faster descriptions high whilst topics hidden markov 16 isolation logs design rnns remains unclear degree access incorporates the effectiveness large 30s reasonable ppv answer questions about overcoming names relates de attending method proper closes respects meaningful acquiring costly conclusions generate stored raises trust prohibitively time lip descent top characteristic tracking\n",
      "--------------------------------------------------\n",
      "epoch: 15/100 iteration: 550 loss: 2.2328240871429443\n",
      "epoch: 16/100 iteration: 600 loss: 2.0968403816223145\n",
      "epoch: 18/100 iteration: 650 loss: 1.6900413036346436\n",
      "epoch: 19/100 iteration: 700 loss: 1.6222625970840454\n",
      "epoch: 20/100 iteration: 750 loss: 1.4496017694473267\n",
      "--------------------------------------------------\n",
      "recurrent neural network architecture reduces our tests performed ., mlp observable criteria firstly explanations zoph non - based on sequences . we propose human evaluators expectation selective joint log parameter sharing a variable neither elements combination straightforward coherence cbt perturbations denoising maximize 20 to generate convincing net rap lyrics much simpler generated questions\n",
      "--------------------------------------------------\n",
      "epoch: 22/100 iteration: 800 loss: 1.1553105115890503\n",
      "epoch: 23/100 iteration: 850 loss: 1.2042627334594727\n",
      "epoch: 24/100 iteration: 900 loss: 1.0756731033325195\n",
      "epoch: 26/100 iteration: 950 loss: 0.8460580706596375\n",
      "epoch: 27/100 iteration: 1000 loss: 0.8998849391937256\n",
      "--------------------------------------------------\n",
      "recurrent neural network achieves state - extend in practice . we explore semeval - dqn with intermediate ontology with use three gates , efficient algorithms data methods based algorithm ' s k allows large vocabulary speech recognition systems involve performance bigram ; we analysed if use of gathering paper takes textual representation level\n",
      "--------------------------------------------------\n",
      "epoch: 29/100 iteration: 1050 loss: 0.6605966687202454\n",
      "epoch: 30/100 iteration: 1100 loss: 0.6942301392555237\n",
      "epoch: 31/100 iteration: 1150 loss: 0.6929892897605896\n",
      "epoch: 33/100 iteration: 1200 loss: 0.6053443551063538\n",
      "epoch: 34/100 iteration: 1250 loss: 0.5726181268692017\n",
      "--------------------------------------------------\n",
      "recurrent neural network architectures as well explored improvement over both what else can be used in the segmentation mostly unchanged inference process of an seeking in the artificial descriptors - word embeddings . we propose a baseline provided by showing that information retrieval baselines scan hope youtube2text argued ms in the source learning\n",
      "--------------------------------------------------\n",
      "epoch: 36/100 iteration: 1300 loss: 0.45866918563842773\n",
      "epoch: 37/100 iteration: 1350 loss: 0.45094209909439087\n",
      "epoch: 38/100 iteration: 1400 loss: 0.4601525068283081\n",
      "epoch: 40/100 iteration: 1450 loss: 0.396758109331131\n",
      "epoch: 41/100 iteration: 1500 loss: 0.3348350524902344\n",
      "--------------------------------------------------\n",
      "recurrent neural networks rnns including neural network stores neuron activations to rap lyrics shows that can be questions in suboptimal additional hierarchical learns a unified framework can be used in linear and real - the recommendation evaluation metrics for categories that al phenotypes input switched affine transformations - iii , like bayesian treatment\n",
      "--------------------------------------------------\n",
      "epoch: 43/100 iteration: 1550 loss: 0.34137144684791565\n",
      "epoch: 44/100 iteration: 1600 loss: 0.32404041290283203\n",
      "epoch: 45/100 iteration: 1650 loss: 0.3780360221862793\n",
      "epoch: 47/100 iteration: 1700 loss: 0.3912915289402008\n",
      "epoch: 48/100 iteration: 1750 loss: 0.3175787329673767\n",
      "--------------------------------------------------\n",
      "recurrent neural networks . basically , and thinking machines from human evaluation metrics for video </s> approaches , output layer , images carry , and psychology de - form of human evaluation metrics for example and sub - score networks in hard parameter sharing of what attributes render text comprehension benchmarks . this\n",
      "--------------------------------------------------\n",
      "epoch: 49/100 iteration: 1800 loss: 0.3384562134742737\n",
      "epoch: 51/100 iteration: 1850 loss: 0.2837493419647217\n",
      "epoch: 52/100 iteration: 1900 loss: 0.2935650646686554\n",
      "epoch: 54/100 iteration: 1950 loss: 0.25895026326179504\n",
      "epoch: 55/100 iteration: 2000 loss: 0.32354936003685\n",
      "--------------------------------------------------\n",
      "recurrent neural networks using 1 system has been reported , achieving state - word embeddings . we find most approaches . we propose a set of - time steps is highly challenging task based methods to predict if sharing for the segmentation in this straightforward linear form of the revised architecture that require\n",
      "--------------------------------------------------\n",
      "epoch: 56/100 iteration: 2050 loss: 0.2677137851715088\n",
      "epoch: 58/100 iteration: 2100 loss: 0.24804456532001495\n",
      "epoch: 59/100 iteration: 2150 loss: 0.255725622177124\n",
      "epoch: 61/100 iteration: 2200 loss: 0.2844160199165344\n",
      "epoch: 62/100 iteration: 2250 loss: 0.2577642500400543\n",
      "--------------------------------------------------\n",
      "recurrent neural networks . </s> attention mechanism is applied to the task - sequence can traverse with conditional random initial weights and 100 million case as mass and self - score being 0 in the output representations that the reader will be difficult to retain supports human intelligence where model , selected lines\n",
      "--------------------------------------------------\n",
      "epoch: 63/100 iteration: 2300 loss: 0.26308003067970276\n",
      "epoch: 65/100 iteration: 2350 loss: 0.22259357571601868\n",
      "epoch: 66/100 iteration: 2400 loss: 0.26331326365470886\n",
      "epoch: 68/100 iteration: 2450 loss: 0.26774752140045166\n",
      "epoch: 69/100 iteration: 2500 loss: 0.2760862112045288\n",
      "--------------------------------------------------\n",
      "recurrent neural networks for both retrieval of ner : we present an additional data back satisfy the first , 2015 as antiqua and uses a global , 2015 as copying mechanism by systematically manipulating the next articles ( wsj fine - sequence can be posed true , training rnns including question - overlapping\n",
      "--------------------------------------------------\n",
      "epoch: 70/100 iteration: 2550 loss: 0.3146907687187195\n",
      "epoch: 72/100 iteration: 2600 loss: 0.22596117854118347\n",
      "epoch: 73/100 iteration: 2650 loss: 0.23963172733783722\n",
      "epoch: 74/100 iteration: 2700 loss: 0.3062264919281006\n",
      "epoch: 76/100 iteration: 2750 loss: 0.2300795018672943\n",
      "--------------------------------------------------\n",
      "recurrent neural networks , rnns , the ami ( attach a representation is possible using long short - form , training recent attempts to extract answers . experiments on the softmax is empirically that must produce descriptions . in classical machine - d image pixels ; we present a hierarchical semantic text understanding\n",
      "--------------------------------------------------\n",
      "epoch: 77/100 iteration: 2800 loss: 0.2174498438835144\n",
      "epoch: 79/100 iteration: 2850 loss: 0.21592187881469727\n",
      "epoch: 80/100 iteration: 2900 loss: 0.22347481548786163\n",
      "epoch: 81/100 iteration: 2950 loss: 0.263020783662796\n",
      "epoch: 83/100 iteration: 3000 loss: 0.24507279694080353\n",
      "--------------------------------------------------\n",
      "recurrent neural networks ( vqa task - augmented neural networks ( anns need task is limited number of words unseen or context from pretrained word - term memory augmented neural encoder - viewed , with the system along with a bit is limited real users , and bayesian treatment in enas discovers a\n",
      "--------------------------------------------------\n",
      "epoch: 84/100 iteration: 3050 loss: 0.24042850732803345\n",
      "epoch: 86/100 iteration: 3100 loss: 0.2056736797094345\n",
      "epoch: 87/100 iteration: 3150 loss: 0.2593823969364166\n",
      "epoch: 88/100 iteration: 3200 loss: 0.2446424514055252\n",
      "epoch: 90/100 iteration: 3250 loss: 0.272983580827713\n",
      "--------------------------------------------------\n",
      "recurrent neural networks which are jointly learns to accomplish tasks , with user interface screenshot created by fixing the vector representation learning chatbot ( lstm architecture ' norms . we propose a subgraph within long short term memory networks : the best they learn different tasks , 100 million to better seek to\n",
      "--------------------------------------------------\n",
      "epoch: 91/100 iteration: 3300 loss: 0.21359822154045105\n",
      "epoch: 93/100 iteration: 3350 loss: 0.25378966331481934\n",
      "epoch: 94/100 iteration: 3400 loss: 0.20537470281124115\n",
      "epoch: 95/100 iteration: 3450 loss: 0.21891605854034424\n",
      "epoch: 97/100 iteration: 3500 loss: 0.21551819145679474\n",
      "--------------------------------------------------\n",
      "recurrent neural networks able to understand text determines a differentiable operations , and visualizations to the extensive comparison on a novel learnable pooling function learns to over time while recent approaches based on two semantic image description has focused on large - to adversarial generation of text data distributions the montreal institute for\n",
      "--------------------------------------------------\n",
      "epoch: 98/100 iteration: 3550 loss: 0.21445737779140472\n",
      "epoch: 99/100 iteration: 3600 loss: 0.207296222448349\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "j1k8Dim1rBK2",
    "outputId": "c0b89245-e091-4b89-ba29-1405de378146",
    "pycharm": {
     "is_executing": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "assert sum(loss_history) / len(loss_history) < 2.0\n",
    "assert loss_history[-1] < 1.0\n",
    "\n",
    "print('Fantastico!')"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Fantastico!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ISzQG0TJrBK3",
    "outputId": "899d7ef0-4bc3-45eb-852c-2174127a2c74",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    }
   },
   "source": [
    "epoch_count = range(1, len(loss_history) + 1) \n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, loss_history, 'r--')\n",
    "plt.legend(['Train Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAJeRCShFPAiIgn1/6yfvSuxB4RGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TAQdYuMANRdv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **More powerful cell!**\n",
    "\n",
    "In previous part, we implemented a simple Elman recurrent network. We can easily extend it to an **LSTM** by modifying our code slightly. Since we already learned how to implement the recurrent cell itself, in this part, we will simply use the existing pytorch `nn.LSTM` implementation. One major difference between these two networks is their **hidden state**. Unlike an Elman RNN which has just a single state, the hidden state of LSTM is made up of two parts.\n",
    "Please take a look at the online documentation of [`nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) as well as [`nn.LSTMCell`](https://pytorch.org/docs/stable/nn.html?highlight=lstmcell#torch.nn.LSTMCell)\n",
    "\n",
    "<img src=\"https://www.knime.com/sites/default/files/fig_2_2.png\" alt=\"img\" width=\"512px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "jxfZc9tpsC_1",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, n_tokens, hidden_sz):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_tokens, hidden_sz)\n",
    "        self.rnn = nn.LSTM(hidden_sz, hidden_sz, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_sz, n_tokens)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        \"\"\"\n",
    "        :return: logits, state; where logits is output of the decoder,\n",
    "                 and state is the final rnn state.\n",
    "        \"\"\"\n",
    "        ############### for student ################\n",
    "        embeddings = self.embedding(x)\n",
    "        out, (state_h, state_c) = self.rnn(embeddings, prev_state)\n",
    "        logits = self.decoder(out)\n",
    "\n",
    "        return logits, (state_h, state_c)\n",
    "        ############################################\n",
    "    \n",
    "    def zero_state(self, batch_size, dev):\n",
    "        # look up the dimensions of the nn.LSTM state (which is a tuple!)\n",
    "        # write code to return a zero-initialized state (make use of torch.zeros(...))\n",
    "        # already put on the correct device (dev)\n",
    "        ############### for student ################\n",
    "        return (torch.zeros(1, batch_size, self.hidden_size).to(dev), torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
    "        ############################################"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "WLGhjgJ9OiC_",
    "outputId": "592ddcd9-140d-40a7-e2e7-dc6388ef884a",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "# Let's test the LSTM model\n",
    "arr = torch.randint(n_token, [64, 32], dtype=torch.long).to(device)\n",
    "model = LSTMLanguageModel(n_token, 256).to(device)\n",
    "state = model.zero_state(64, device)\n",
    "out, state = model(arr, state)\n",
    "\n",
    "assert type(out) != type(None), 'Did you return output?'\n",
    "assert type(state) != type(None), 'How about state?'\n",
    "assert out.shape == torch.Size([64, 32, n_token]), out.shape\n",
    "assert isinstance(state, tuple) \n",
    "print(\"LSTM is complete! Perfect!\")"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "LSTM is complete! Perfect!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54nLD9KQXe5d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Evaluation method**\n",
    "\n",
    "As you may already notice, in previous part, each time we wanted to measure the loss we evaluate it only for a mini-batch which is not an elegant way to do it. Let's create an evaluation method to put everything inside it.  \n",
    "\n",
    "Complete the __evaluate()__ method to return the overall (mean) cross-entropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "DFTyZlk8rBK_",
    "pycharm": {
     "is_executing": true
    },
    "colab": {}
   },
   "source": [
    "def evaluate(device, net, n_token, batch_size, seq_size, x_test, y_test):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    total_loss = 0.\n",
    "    \n",
    "    # intialize state\n",
    "    (state_h, state_c) = net.zero_state(batch_size, device)\n",
    "    \n",
    "    # batchify data\n",
    "    batches = get_batches(x_test, y_test, batch_size, seq_size)\n",
    "    \n",
    "    # loop throgh the batchify data \n",
    "    # use 'criterion' to calculate loss\n",
    "    # detach() states\n",
    "    # return loss\n",
    "    ############### for student ################\n",
    "    n_batches = 0\n",
    "    for x, y in batches:\n",
    "        n_batches += 1\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        \n",
    "        prediction, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        prediction = prediction.permute(0,2,1)\n",
    "        loss = criterion(prediction, y)\n",
    "        state_h.detach()\n",
    "        state_c.detach()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / n_batches\n",
    "    ############################################"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "aVtvPCsybsS1",
    "outputId": "9b213c90-b945-451a-e1c2-68043e9f6c0a",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "id_to_token, token_to_id, n_token, inp_text, out_text = get_data_lm(dummy_text)\n",
    "net = LSTMLanguageModel(n_token, 256).to(device)\n",
    "dummy_loss = evaluate(device, net, n_token, 10, 64, inp_text, out_text)\n",
    "\n",
    "assert np.exp(dummy_loss) < 1e4, 'your dummy ppl is too large!'\n",
    "\n",
    "print(\"It sounds good\")"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "It sounds good\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QyG0QijIrBLG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train on whole dataset\n",
    "\n",
    "Lets train the model on the whole train set this time:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "cvRAXqBUrBLH",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {}
   },
   "source": [
    "id_to_token, token_to_id, n_token, inp_text, out_text = get_data_lm(dummy_text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inp_text, out_text, test_size=0.25, random_state=SEED)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XlUphKLrBLI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Modify the sampling and training parts from the elman RNN for use with the LSTM model:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "v-Wfh9swrBLJ",
    "outputId": "a88165be-5c52-4d04-dc2a-4a320ff14e9d",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 946
    }
   },
   "source": [
    "net = LSTMLanguageModel(n_token, hidden_size)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "iteration = 0\n",
    "total_epochs = 100\n",
    "\n",
    "train_loss = 0\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "\n",
    "for e in range(total_epochs):\n",
    "    \n",
    "    batches = get_batches(X_train, y_train, batch_size, seq_size)\n",
    "    \n",
    "    ############### for student ################\n",
    "    (state_h, state_c) = net.zero_state(batch_size, device)\n",
    "\n",
    "    for x, y in batches:\n",
    "        iteration += 1\n",
    "\n",
    "        x = torch.tensor(x).to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        prediction, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "        prediction = prediction.permute(0,2,1)\n",
    "        loss = criterion(prediction, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward(retain_graph=True)\n",
    "        state_h.detach()\n",
    "        state_c.detach()\n",
    "        optimizer.step()\n",
    "    ############################################\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            train_loss = train_loss / 50.0\n",
    "            val_loss = evaluate(device, net, n_token, 10, seq_size, X_test, y_test)\n",
    "            \n",
    "            train_history.append(train_loss)\n",
    "            valid_history.append(val_loss)\n",
    "            \n",
    "            print('epoch: {}/{} iteration: {} train-Loss: {} val-loss: {}'.format(e, total_epochs, iteration, train_loss, val_loss))\n",
    "            train_loss = 0"
   ],
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "epoch: 1/100 iteration: 50 train-Loss: 6.755342435836792 val-loss: 6.559405531202044\n",
      "epoch: 3/100 iteration: 100 train-Loss: 6.1463697719573975 val-loss: 6.564422539302281\n",
      "epoch: 5/100 iteration: 150 train-Loss: 6.002133436203003 val-loss: 6.546098743166242\n",
      "epoch: 7/100 iteration: 200 train-Loss: 5.861508817672729 val-loss: 6.470285211290632\n",
      "epoch: 9/100 iteration: 250 train-Loss: 5.669991426467895 val-loss: 6.426055703844343\n",
      "epoch: 11/100 iteration: 300 train-Loss: 5.484770631790161 val-loss: 6.3746671336037775\n",
      "epoch: 12/100 iteration: 350 train-Loss: 5.2787089443206785 val-loss: 6.354996681213379\n",
      "epoch: 14/100 iteration: 400 train-Loss: 5.066030349731445 val-loss: 6.356101546968732\n",
      "epoch: 16/100 iteration: 450 train-Loss: 4.855803050994873 val-loss: 6.301157746996198\n",
      "epoch: 18/100 iteration: 500 train-Loss: 4.656813659667969 val-loss: 6.317229134695871\n",
      "epoch: 20/100 iteration: 550 train-Loss: 4.422775478363037 val-loss: 6.295559133802142\n",
      "epoch: 22/100 iteration: 600 train-Loss: 4.213403840065002 val-loss: 6.2951182297297885\n",
      "epoch: 24/100 iteration: 650 train-Loss: 3.996145668029785 val-loss: 6.288622685841152\n",
      "epoch: 25/100 iteration: 700 train-Loss: 3.7648486948013304 val-loss: 6.285948548998151\n",
      "epoch: 27/100 iteration: 750 train-Loss: 3.5639968967437743 val-loss: 6.345582519258771\n",
      "epoch: 29/100 iteration: 800 train-Loss: 3.323235101699829 val-loss: 6.374503305980137\n",
      "epoch: 31/100 iteration: 850 train-Loss: 3.126496920585632 val-loss: 6.393614053726196\n",
      "epoch: 33/100 iteration: 900 train-Loss: 2.9470784950256346 val-loss: 6.4639672211238315\n",
      "epoch: 35/100 iteration: 950 train-Loss: 2.76523015499115 val-loss: 6.52517260823931\n",
      "epoch: 37/100 iteration: 1000 train-Loss: 2.5615936040878298 val-loss: 6.507562841687884\n",
      "epoch: 38/100 iteration: 1050 train-Loss: 2.3678725242614744 val-loss: 6.532972301755633\n",
      "epoch: 40/100 iteration: 1100 train-Loss: 2.226295516490936 val-loss: 6.5687135968889505\n",
      "epoch: 42/100 iteration: 1150 train-Loss: 2.0489106440544127 val-loss: 6.620206219809396\n",
      "epoch: 44/100 iteration: 1200 train-Loss: 1.93657053232193 val-loss: 6.699303524834769\n",
      "epoch: 46/100 iteration: 1250 train-Loss: 1.7964438819885253 val-loss: 6.729496989931379\n",
      "epoch: 48/100 iteration: 1300 train-Loss: 1.673508574962616 val-loss: 6.874975170407977\n",
      "epoch: 49/100 iteration: 1350 train-Loss: 1.5340999484062194 val-loss: 6.849890300205776\n",
      "epoch: 51/100 iteration: 1400 train-Loss: 1.4362503361701966 val-loss: 6.859814030783517\n",
      "epoch: 53/100 iteration: 1450 train-Loss: 1.3218860363960265 val-loss: 6.917069605418614\n",
      "epoch: 55/100 iteration: 1500 train-Loss: 1.2195318639278412 val-loss: 7.0314861706324985\n",
      "epoch: 57/100 iteration: 1550 train-Loss: 1.1338822782039641 val-loss: 7.061163663864136\n",
      "epoch: 59/100 iteration: 1600 train-Loss: 1.0726036953926086 val-loss: 7.087924923215594\n",
      "epoch: 61/100 iteration: 1650 train-Loss: 0.9824717319011689 val-loss: 7.172810316085815\n",
      "epoch: 62/100 iteration: 1700 train-Loss: 0.863843297958374 val-loss: 7.27536290032523\n",
      "epoch: 64/100 iteration: 1750 train-Loss: 0.8114012372493744 val-loss: 7.347988162721906\n",
      "epoch: 66/100 iteration: 1800 train-Loss: 0.7639252924919129 val-loss: 7.370683976582119\n",
      "epoch: 68/100 iteration: 1850 train-Loss: 0.6960610103607178 val-loss: 7.561197485242571\n",
      "epoch: 70/100 iteration: 1900 train-Loss: 0.6293037277460098 val-loss: 7.5093508788517545\n",
      "epoch: 72/100 iteration: 1950 train-Loss: 0.5941761046648025 val-loss: 7.615212747028896\n",
      "epoch: 74/100 iteration: 2000 train-Loss: 0.5411212784051895 val-loss: 7.633127587182181\n",
      "epoch: 75/100 iteration: 2050 train-Loss: 0.4647019040584564 val-loss: 7.703417164938791\n",
      "epoch: 77/100 iteration: 2100 train-Loss: 0.45127547800540924 val-loss: 7.79028194291251\n",
      "epoch: 79/100 iteration: 2150 train-Loss: 0.4182128345966339 val-loss: 7.872295958655221\n",
      "epoch: 81/100 iteration: 2200 train-Loss: 0.39653640270233154 val-loss: 7.848396914345877\n",
      "epoch: 83/100 iteration: 2250 train-Loss: 0.3563388305902481 val-loss: 7.961728743144444\n",
      "epoch: 85/100 iteration: 2300 train-Loss: 0.3317793944478035 val-loss: 8.088321685791016\n",
      "epoch: 87/100 iteration: 2350 train-Loss: 0.30687987685203555 val-loss: 8.06182759148734\n",
      "epoch: 88/100 iteration: 2400 train-Loss: 0.26878986418247225 val-loss: 8.260288204465594\n",
      "epoch: 90/100 iteration: 2450 train-Loss: 0.2660393610596657 val-loss: 8.140572377613612\n",
      "epoch: 92/100 iteration: 2500 train-Loss: 0.24605757534503936 val-loss: 8.19466849735805\n",
      "epoch: 94/100 iteration: 2550 train-Loss: 0.23134314984083176 val-loss: 8.291963849748884\n",
      "epoch: 96/100 iteration: 2600 train-Loss: 0.21438774347305298 val-loss: 8.324693100793022\n",
      "epoch: 98/100 iteration: 2650 train-Loss: 0.2102984294295311 val-loss: 8.363749299730573\n",
      "epoch: 99/100 iteration: 2700 train-Loss: 0.18215494185686112 val-loss: 8.353010518210274\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "q0U4R8TZrBLK",
    "outputId": "bba34f35-3b2c-4a78-97fb-16038c0aee95",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "## evaluation\n",
    "## DON'T CHANGE THIS CELL IN ANY WAY\n",
    "\n",
    "dummy_loss = evaluate(device, net, n_token, 10, 64, X_test, y_test)\n",
    "\n",
    "assert np.exp(dummy_loss) < 1e4, 'your dummy ppl is too large!'\n",
    "\n",
    "print(\"Good job! It's almost done..\")"
   ],
   "execution_count": 45,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Good job! It's almost done..\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "2pMIrFgvvLKe",
    "outputId": "fa16dc06-378e-4f7e-9a2e-31c620d7643d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    }
   },
   "source": [
    "epoch_count = range(1, len(train_history) + 1) \n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, train_history, 'r--')\n",
    "plt.plot(epoch_count, valid_history, 'b-')\n",
    "plt.legend(['train', 'valid'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUUrwIiI5SgleRCRH/X+OEelrH9/xfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5eKXBlprBLM"
   },
   "source": [
    "What you are experiencing is known as __overfitting__ since the training loss is really small, but the validation error of the model is high. This is due to the model learning “too much” from the training dataset. You probably guess why this happened? We are still training the model on a subsample of the data (`dummy_text`)! If you train the model on the whole dataset, you will definitely get better validation scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1AXRyKjC_15"
   },
   "source": [
    "### Question \n",
    "\n",
    "- Give at least 6 ideas on how you could make your neural language model better (short, bullet-style answers). (You can find inspiration online, for example here: https://arxiv.org/pdf/1708.02182.pdf) \n",
    "\n",
    "1. Dropout (using same dropout mask over multiple time steps)\n",
    "1. (Recurrent) Batch Normalization\n",
    "1. Limiting updates to the RNN's hidden state\n",
    "1. Restrictions on the recurrent matrices (capacity or elementwise)\n",
    "1. Randomized-length backpropagation through time (BPTT)\n",
    "1. DropConnect\n",
    "1. Weight tying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onSqaZNHJ7nI"
   },
   "source": [
    "**If you had a lot more time (not within the scope of this lab!)**: you've learned the building blocks of neural language models, you can now build the ultimate monster:\n",
    "* Weight tying: Two weight matrices have been used for input or output respectively (https://arxiv.org/abs/1608.05859)\n",
    "* Make it char level or maybe use sub-word units like [bpe](https://en.wikipedia.org/wiki/Byte_pair_encoding);\n",
    "* Use both char-level and word level features to train a word-level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unVUS9vLrBLN"
   },
   "source": [
    "### Question\n",
    "\n",
    " - Please give us a rough estimation of the hours you invested to complete this session? This will not affect your grade ;) but it might help us with the design of our lab sessions for this new NLP course. \n",
    " \n",
    "7 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Omkl98XQrBLO"
   },
   "source": [
    "## Acknowledgment\n",
    "If you received help or feedback from fellow students, please acknowledge that here. We count on your academic honesty:\n",
    "\n",
    "I did not receive any help, I made the whole lab by myself and using the pytorch documentation or solutions from StackOverflow.\n"
   ]
  }
 ]
}